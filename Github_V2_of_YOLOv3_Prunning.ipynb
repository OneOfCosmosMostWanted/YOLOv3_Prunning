{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "g8DPsAM0Zv5p",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Load Dataset\n",
        "\n",
        "!kaggle datasets download -d debeshjha1/kvasirseg\n",
        "\n",
        "!unzip kvasirseg.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OMwS8-5Z6g-"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import shutil\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set random seeds for full reproducibility\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLHOyWrgaPhQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Config\n",
        "\n",
        "DATASET = \"Kvasir-SEG\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_CLASSES = 1\n",
        "CHECKPOINT_FILE = \"/content/drive/MyDrive/models/checkpoint.pth.tar\" # pruned_checkpoint\n",
        "IMG_DIR = \"/content/Kvasir-SEG/Kvasir-SEG/images/\"\n",
        "LABEL_DIR = \"/content/Kvasir-SEG/Kvasir-SEG/bbox/\"\n",
        "DATASET_DIR = \"/content/Kvasir-SEG/Kvasir-SEG/\"\n",
        "\n",
        "CONF_THRESHOLD = 0.6\n",
        "MAP_IOU_THRESH = 0.5\n",
        "NMS_IOU_THRESH = 0.45\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "BATCH_SIZE = 30\n",
        "NUM_EPOCHS = 100000000\n",
        "NUM_WORKERS = 4\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = False\n",
        "\n",
        "CRITERION = \"L1\"\n",
        "\n",
        "split_pct = 0.1\n",
        "IMAGE_SIZE = 416\n",
        "\n",
        "S = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n",
        "\n",
        "img_lst = os.listdir(IMG_DIR)\n",
        "label_lst = os.listdir(LABEL_DIR)\n",
        "\n",
        "sorted_img_lst = sorted(img_lst)\n",
        "sorted_label_lst = sorted(label_lst)\n",
        "\n",
        "ANCHORS = [\n",
        "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "]  # Note these have been rescaled to be between [0, 1]\n",
        "\n",
        "# ANCHORS = [[(0.87, 0.91), (0.61, 0.75), (0.57, 0.5)], [(0.45, 0.63), (0.4, 0.45), (0.28, 0.47)], [(0.29, 0.32), (0.19, 0.24), (0.09, 0.11)]]\n",
        "\n",
        "KVASIR_CLASSES = [\n",
        "    \"polyp\"\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TpDXcSpngdRj"
      },
      "outputs": [],
      "source": [
        "# @title K-Means Clustering (Anchors)\n",
        "def iou(box, clusters):\n",
        "    \"\"\"\n",
        "    Calculate the IoU between a box and k clusters (anchor boxes).\n",
        "    \"\"\"\n",
        "    x = np.minimum(clusters[:, 0], box[0])\n",
        "    y = np.minimum(clusters[:, 1], box[1])\n",
        "\n",
        "    if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:\n",
        "        raise ValueError(\"Box has no area\")\n",
        "\n",
        "    intersection = x * y\n",
        "    box_area = box[0] * box[1]\n",
        "    cluster_area = clusters[:, 0] * clusters[:, 1]\n",
        "\n",
        "    union = box_area + cluster_area - intersection\n",
        "    return intersection / union\n",
        "\n",
        "def kmeans_anchors(boxes, k, dist_fn=iou, max_iter=10000):\n",
        "    \"\"\"\n",
        "    K-Means clustering with IoU-based distance for anchor box generation.\n",
        "\n",
        "    Parameters:\n",
        "    - boxes: A numpy array of shape (num_boxes, 2), where each row is [width, height].\n",
        "    - k: The number of anchors (clusters).\n",
        "    - dist_fn: The distance function, which should be IoU in this case.\n",
        "    - max_iter: Maximum number of iterations for the K-Means algorithm.\n",
        "\n",
        "    Returns:\n",
        "    - anchors: A numpy array of shape (k, 2) containing the optimized anchor boxes (width, height).\n",
        "    \"\"\"\n",
        "    # Initialize the clusters randomly by choosing k boxes from the dataset\n",
        "    indices = np.random.choice(boxes.shape[0], k, replace=False)\n",
        "    clusters = boxes[indices]\n",
        "    count = 0\n",
        "    for iteration in range(max_iter):\n",
        "        print(count)\n",
        "        count += 1\n",
        "        # Assign each box to the closest cluster (highest IoU)\n",
        "        distances = np.array([1 - dist_fn(box, clusters) for box in boxes])\n",
        "        nearest_clusters = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Recalculate clusters as the mean of all boxes assigned to them\n",
        "        new_clusters = np.array([boxes[nearest_clusters == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "        # Check for convergence (if clusters do not change)\n",
        "        if np.all(clusters == new_clusters):\n",
        "            print(\"break\")\n",
        "            break\n",
        "        clusters = new_clusters\n",
        "\n",
        "    return clusters\n",
        "\n",
        "def get_bounding_boxes(img_lst, label_lst, print_examples=False):\n",
        "    \"\"\"\n",
        "    Extract bounding box widths and heights from a list of images and their corresponding label files.\n",
        "\n",
        "    Parameters:\n",
        "    - img_lst: List of image filenames.\n",
        "    - label_lst: List of corresponding label filenames (CSV).\n",
        "    - print_examples: Boolean flag to print out width/height examples.\n",
        "\n",
        "    Returns:\n",
        "    - A numpy array of shape (num_boxes, 2), where each row contains [width, height] for each bounding box.\n",
        "    \"\"\"\n",
        "    boxes = []\n",
        "\n",
        "    # Loop through each image and label file\n",
        "    for i in range(len(label_lst)):\n",
        "        img_path = os.path.join(IMG_DIR, img_lst[i])\n",
        "        label_path = os.path.join(LABEL_DIR, label_lst[i])\n",
        "\n",
        "        # Load the image to get its dimensions\n",
        "        img = Image.open(img_path)\n",
        "        image_width, image_height = img.size\n",
        "\n",
        "        # Read the CSV label file containing bounding box data\n",
        "        df = pd.read_csv(label_path)\n",
        "\n",
        "        # Extract bounding box coordinates and class name\n",
        "        bounding_boxes = df[['class_name', 'xmin', 'ymin', 'xmax', 'ymax']].to_numpy()\n",
        "\n",
        "        # Loop through bounding boxes and compute width and height\n",
        "        for bbox in bounding_boxes:\n",
        "            class_name, xmin, ymin, xmax, ymax = bbox\n",
        "\n",
        "            # Calculate width and height of the bounding box\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "\n",
        "            # Normalize width and height relative to the image size\n",
        "            normalized_width = width / image_width\n",
        "            normalized_height = height / image_height\n",
        "\n",
        "            # Optionally print a few examples\n",
        "            if print_examples and len(boxes) < 5:  # Print only the first few examples\n",
        "                fig, ax = plt.subplots()\n",
        "\n",
        "                ax.imshow(img)\n",
        "                # Plot each bounding box\n",
        "                for index, row in df.iterrows():\n",
        "                    x_min, y_min, x_max, y_max = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "                    width = x_max - x_min\n",
        "                    height = y_max - y_min\n",
        "                    # Create a rectangle patch\n",
        "                    rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "                    # Add the patch to the axis\n",
        "                    ax.add_patch(rect)\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "                plt.show()\n",
        "                print(label_path)\n",
        "                print(\"image size: \", img.size)\n",
        "                print(f\"Original Width: {width}, Original Height: {height}\")\n",
        "                print(f\"Normalized Width: {normalized_width}, Normalized Height: {normalized_height}\")\n",
        "\n",
        "            # Append the [width, height] to the list\n",
        "            boxes.append([normalized_width, normalized_height])\n",
        "\n",
        "    # Convert the list to a numpy array of shape (num_boxes, 2)\n",
        "    return np.array(boxes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get the bounding box widths and heights\n",
        "bounding_boxes = get_bounding_boxes(sorted_img_lst, sorted_label_lst, print_examples=True)\n",
        "print(len(bounding_boxes))\n",
        "print(len(label_lst))\n",
        "\n",
        "# Perform IoU-based K-Means clustering to get optimized anchor boxes\n",
        "anchors = kmeans_anchors(bounding_boxes, k=9)\n",
        "\n",
        "print(f\"Optimized Anchors (width, height):\\n{anchors}\")\n",
        "\n",
        "areas = np.prod(anchors, axis=1)  # Calculate the area (width * height)\n",
        "print(areas)\n",
        "sorted_indices = np.argsort(-areas)  # Sort indices based on area\n",
        "sorted_anchors = anchors[sorted_indices]\n",
        "# Rearrange into the desired format (largest anchors first, grouped into 3 per scale)\n",
        "formatted_anchors = [\n",
        "    list(map(tuple, sorted_anchors[:3])),  # Largest anchors\n",
        "    list(map(tuple, sorted_anchors[3:6])), # Medium anchors\n",
        "    list(map(tuple, sorted_anchors[6:]))   # Smallest anchors\n",
        "]\n",
        "formatted_anchors = [[(float(x), float(y)) for (x, y) in row] for row in formatted_anchors]\n",
        "print(formatted_anchors)\n",
        "\n",
        "ANCHORS = formatted_anchors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZK9efnHyafIn"
      },
      "outputs": [],
      "source": [
        "# @title Transforms\n",
        "train_transform = A.Compose([A.Resize(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                             A.Rotate(limit=15,p=0.1),\n",
        "                             A.HorizontalFlip(p=0.5),\n",
        "                             A.Normalize(mean=(0,0,0),std=(1,1,1),max_pixel_value=255),\n",
        "                             ToTensorV2()])\n",
        "\n",
        "val_transform = A.Compose([A.Resize(IMAGE_SIZE,IMAGE_SIZE),\n",
        "                           A.Normalize(mean=(0,0,0),std=(1,1,1),max_pixel_value=255),\n",
        "                           ToTensorV2()])\n",
        "\n",
        "scale = 1.1\n",
        "train_transforms = A.Compose(\n",
        "    [\n",
        "        A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
        "        A.PadIfNeeded(\n",
        "            min_height=int(IMAGE_SIZE * scale),\n",
        "            min_width=int(IMAGE_SIZE * scale),\n",
        "            border_mode=cv2.BORDER_CONSTANT,\n",
        "            value=0,\n",
        "        ),\n",
        "        A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
        "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
        "        A.OneOf(\n",
        "            [\n",
        "                A.ShiftScaleRotate(\n",
        "                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
        "                ),\n",
        "                A.Affine(shear=15, p=0.5),\n",
        "            ],\n",
        "            p=1.0,\n",
        "        ),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Blur(p=0.1),\n",
        "        A.CLAHE(p=0.1),\n",
        "        A.Posterize(p=0.1),\n",
        "        A.ToGray(p=0.1),\n",
        "        A.ChannelShuffle(p=0.05),\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
        ")\n",
        "\n",
        "test_transforms = A.Compose(\n",
        "    [\n",
        "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
        "        A.PadIfNeeded(\n",
        "            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT, value=0\n",
        "        ),\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VpxISBRia3BA"
      },
      "outputs": [],
      "source": [
        "# @title Show Image\n",
        "\n",
        "def show_images(img_lst, label_lst, loops=10):\n",
        "    for i in range(loops):\n",
        "        img_path = os.path.join(IMG_DIR, img_lst[i])\n",
        "        label_path = os.path.join(LABEL_DIR, label_lst[i])\n",
        "        img = Image.open(img_path)\n",
        "        image_width, image_height = img.size\n",
        "\n",
        "        print(img_path)\n",
        "        print(img.size)\n",
        "        print(type(img))\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        print(label_path)\n",
        "\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(label_path)\n",
        "\n",
        "        # Display the dataframe\n",
        "        print(df)\n",
        "\n",
        "        bounding_boxes = df[['class_name', 'xmin', 'ymin', 'xmax', 'ymax']].to_numpy()\n",
        "        print(bounding_boxes)\n",
        "        print(bounding_boxes.shape)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        ax.imshow(img)\n",
        "        # Plot each bounding box\n",
        "        for index, row in df.iterrows():\n",
        "            x_min, y_min, x_max, y_max = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "            width = x_max - x_min\n",
        "            height = y_max - y_min\n",
        "            # Create a rectangle patch\n",
        "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            # Add the patch to the axis\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Assuming 'polyp' is the only class, so class_id = 1\n",
        "        df['class_name'] = 1\n",
        "\n",
        "        # Normalize bounding box coordinates\n",
        "        df['x_center'] = (df['xmin'] + df['xmax']) / 2 / image_width\n",
        "        df['y_center'] = (df['ymin'] + df['ymax']) / 2 / image_height\n",
        "        df['width'] = (df['xmax'] - df['xmin']) / image_width\n",
        "        df['height'] = (df['ymax'] - df['ymin']) / image_height\n",
        "\n",
        "        # Prepare YOLO format data\n",
        "        yolo_format = df[['class_name', 'x_center', 'y_center', 'width', 'height']].to_numpy()\n",
        "\n",
        "        # Display the dataframe\n",
        "        print(df)\n",
        "\n",
        "        print(yolo_format)\n",
        "\n",
        "        print(yolo_format.shape)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(img)\n",
        "        # Create a Rectangle patch\n",
        "        for box in yolo_format:\n",
        "            box = box[1:]\n",
        "            upper_left_x = box[0] - box[2] / 2\n",
        "            upper_left_y = box[1] - box[3] / 2\n",
        "            rect = patches.Rectangle(\n",
        "                (upper_left_x * image_width, upper_left_y * image_height),\n",
        "                box[2] * image_width,\n",
        "                box[3] * image_height,\n",
        "                linewidth=2,\n",
        "                edgecolor='r',\n",
        "                facecolor=\"none\",\n",
        "            )\n",
        "            # Add the patch to the Axes\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(\"----------------------------------------------------\")\n",
        "\n",
        "show_images(sorted_img_lst, sorted_label_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q287LjkBbQ4y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Shuffling Data\n",
        "\n",
        "permuted_train_img_lst = np.random.permutation(np.array(sorted_img_lst))\n",
        "permuted_train_label_lst = [x.replace(\".jpg\", \".csv\") for x in permuted_train_img_lst]\n",
        "print(permuted_train_img_lst[:5])\n",
        "print(permuted_train_label_lst[:5])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "glWvyKULbesc"
      },
      "outputs": [],
      "source": [
        "# @title Splitting into Training and Validation\n",
        "\n",
        "train_images_list = permuted_train_img_lst[int(split_pct*len(permuted_train_img_lst)) :]\n",
        "train_labels_list = permuted_train_label_lst[int(split_pct*len(permuted_train_label_lst)) :]\n",
        "print(len(train_labels_list))\n",
        "\n",
        "val_images_list = permuted_train_img_lst[: int(split_pct*len(permuted_train_img_lst))]\n",
        "val_labels_list = permuted_train_label_lst[: int(split_pct*len(permuted_train_label_lst))]\n",
        "print(len(val_labels_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t-bmZc4sbgD7"
      },
      "outputs": [],
      "source": [
        "# @title Utils for YOLOv3\n",
        "\n",
        "def iou_width_height(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        boxes1 (tensor): width and height of the first bounding boxes\n",
        "        boxes2 (tensor): width and height of the second bounding boxes\n",
        "    Returns:\n",
        "        tensor: Intersection over union of the corresponding boxes\n",
        "    \"\"\"\n",
        "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
        "        boxes1[..., 1], boxes2[..., 1]\n",
        "    )\n",
        "    union = (\n",
        "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
        "    )\n",
        "    return intersection / union\n",
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "    \"\"\"\n",
        "    Video explanation of this function:\n",
        "    https://youtu.be/XXYG5ZWtjj0\n",
        "\n",
        "    This function calculates intersection over union (iou) given pred boxes\n",
        "    and target boxes.\n",
        "\n",
        "    Parameters:\n",
        "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
        "\n",
        "    Returns:\n",
        "        tensor: Intersection over union for all examples\n",
        "    \"\"\"\n",
        "\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "\n",
        "def cells_to_bboxes(predictions, anchors, S, is_preds=True):\n",
        "    \"\"\"\n",
        "    Scales the predictions coming from the model to\n",
        "    be relative to the entire image such that they for example later\n",
        "    can be plotted or.\n",
        "    INPUT:\n",
        "    predictions: tensor of size (N, 3, S, S, num_classes+5)\n",
        "    anchors: the anchors used for the predictions\n",
        "    S: the number of cells the image is divided in on the width (and height)\n",
        "    is_preds: whether the input is predictions or the true bounding boxes\n",
        "    OUTPUT:\n",
        "    converted_bboxes: the converted boxes of sizes (N, num_anchors, S, S, 1+5) with class index,\n",
        "                      object score, bounding box coordinates\n",
        "    \"\"\"\n",
        "    BATCH_SIZE = predictions.shape[0]\n",
        "    num_anchors = len(anchors)\n",
        "    box_predictions = predictions[..., 1:5]\n",
        "    if is_preds:\n",
        "        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
        "        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
        "        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
        "        scores = torch.sigmoid(predictions[..., 0:1])\n",
        "        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
        "    else:\n",
        "        scores = predictions[..., 0:1]\n",
        "        best_class = predictions[..., 5:6]\n",
        "\n",
        "    cell_indices = (\n",
        "        torch.arange(S)\n",
        "        .repeat(predictions.shape[0], 3, S, 1)\n",
        "        .unsqueeze(-1)\n",
        "        .to(predictions.device)\n",
        "    )\n",
        "    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n",
        "    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
        "    w_h = 1 / S * box_predictions[..., 2:4]\n",
        "    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
        "    return converted_bboxes.tolist()\n",
        "\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    \"\"\"\n",
        "    Video explanation of this function:\n",
        "    https://youtu.be/YDkjWEN8jNA\n",
        "\n",
        "    Does Non Max Suppression given bboxes\n",
        "\n",
        "    Parameters:\n",
        "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "\n",
        "    Returns:\n",
        "        list: bboxes after performing NMS given a specific IoU threshold\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            )\n",
        "            < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms\n",
        "\n",
        "def plot_image(image, boxes, target_boxes=None):\n",
        "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
        "    cmap = plt.get_cmap(\"tab20b\")\n",
        "    class_labels = KVASIR_CLASSES\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels)+1)]\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    # Create figure and axes\n",
        "    if target_boxes is not None:\n",
        "      fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "      # Display the image\n",
        "      ax1.imshow(im)\n",
        "      ax2.imshow(im)\n",
        "    else:\n",
        "      # Create figure and axes\n",
        "      fig, ax = plt.subplots(1)\n",
        "      # Display the image\n",
        "      ax.imshow(im)\n",
        "    # box[0] is x midpoint, box[2] is width\n",
        "    # box[1] is y midpoint, box[3] is height\n",
        "\n",
        "    # Plot the target bounding boxes if provided\n",
        "    if target_boxes is not None:\n",
        "        for box in target_boxes:\n",
        "            assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
        "            class_pred = box[0]\n",
        "            box = box[2:]\n",
        "            upper_left_x = box[0] - box[2] / 2\n",
        "            upper_left_y = box[1] - box[3] / 2\n",
        "            rect = patches.Rectangle(\n",
        "                (upper_left_x * width, upper_left_y * height),\n",
        "                box[2] * width,\n",
        "                box[3] * height,\n",
        "                linewidth=5,\n",
        "                edgecolor=colors[1],\n",
        "                facecolor=\"none\",\n",
        "            )\n",
        "            # Add the patch to the Axes\n",
        "            ax1.add_patch(rect)\n",
        "            ax1.text(\n",
        "                upper_left_x * width,\n",
        "                upper_left_y * height,\n",
        "                s='target',\n",
        "                color=\"white\",\n",
        "                verticalalignment=\"top\",\n",
        "                bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
        "            )\n",
        "        # Create a Rectangle patch\n",
        "        for box in boxes:\n",
        "            assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
        "            class_pred = box[0]\n",
        "            box = box[2:]\n",
        "            upper_left_x = box[0] - box[2] / 2\n",
        "            upper_left_y = box[1] - box[3] / 2\n",
        "            rect = patches.Rectangle(\n",
        "                (upper_left_x * width, upper_left_y * height),\n",
        "                box[2] * width,\n",
        "                box[3] * height,\n",
        "                linewidth=2,\n",
        "                edgecolor=colors[int(class_pred)],\n",
        "                facecolor=\"none\",\n",
        "            )\n",
        "            # Add the patch to the Axes\n",
        "            ax2.add_patch(rect)\n",
        "            ax2.text(\n",
        "                upper_left_x * width,\n",
        "                upper_left_y * height,\n",
        "                s=class_labels[int(class_pred)],\n",
        "                color=\"white\",\n",
        "                verticalalignment=\"top\",\n",
        "                bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
        "            )\n",
        "    else:\n",
        "      # Create a Rectangle patch\n",
        "      for box in boxes:\n",
        "          assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
        "          class_pred = box[0]\n",
        "          box = box[2:]\n",
        "          upper_left_x = box[0] - box[2] / 2\n",
        "          upper_left_y = box[1] - box[3] / 2\n",
        "          rect = patches.Rectangle(\n",
        "              (upper_left_x * width, upper_left_y * height),\n",
        "              box[2] * width,\n",
        "              box[3] * height,\n",
        "              linewidth=2,\n",
        "              edgecolor=colors[int(class_pred)],\n",
        "              facecolor=\"none\",\n",
        "          )\n",
        "          # Add the patch to the Axes\n",
        "          ax.add_patch(rect)\n",
        "          plt.text(\n",
        "              upper_left_x * width,\n",
        "              upper_left_y * height,\n",
        "              s=class_labels[int(class_pred)],\n",
        "              color=\"white\",\n",
        "              verticalalignment=\"top\",\n",
        "              bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
        "          )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_loaders(train_images_list, train_labels_list, val_images_list, val_labels_list):\n",
        "\n",
        "    train_dataset = YOLODataset(\n",
        "        train_images_list,\n",
        "        train_labels_list,\n",
        "        transform=train_transforms,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=IMG_DIR,\n",
        "        label_dir=LABEL_DIR,\n",
        "        anchors=ANCHORS,\n",
        "    )\n",
        "    test_dataset = YOLODataset(\n",
        "        val_images_list,\n",
        "        val_labels_list,\n",
        "        transform=test_transforms,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=IMG_DIR,\n",
        "        label_dir=LABEL_DIR,\n",
        "        anchors=ANCHORS,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    train_eval_dataset = YOLODataset(\n",
        "        val_images_list,\n",
        "        val_labels_list,\n",
        "        transform=test_transforms,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=IMG_DIR,\n",
        "        label_dir=LABEL_DIR,\n",
        "        anchors=ANCHORS,\n",
        "    )\n",
        "    train_eval_loader = DataLoader(\n",
        "        dataset=train_eval_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, train_eval_loader\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def check_class_accuracy(model, loader, threshold):\n",
        "    model.eval()\n",
        "    tot_class_preds, correct_class = 0, 0\n",
        "    tot_noobj, correct_noobj = 0, 0\n",
        "    tot_obj, correct_obj = 0, 0\n",
        "\n",
        "    for idx, (x, y) in enumerate(tqdm(loader)):\n",
        "        x = x.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            out = model(x)\n",
        "\n",
        "        for i in range(3):\n",
        "            y[i] = y[i].to(DEVICE)\n",
        "            obj = y[i][..., 0] == 1 # in paper this is Iobj_i\n",
        "            noobj = y[i][..., 0] == 0  # in paper this is Iobj_i\n",
        "\n",
        "            correct_class += torch.sum(\n",
        "                torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n",
        "            )\n",
        "            tot_class_preds += torch.sum(obj)\n",
        "\n",
        "            obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n",
        "            correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n",
        "            tot_obj += torch.sum(obj)\n",
        "            correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n",
        "            tot_noobj += torch.sum(noobj)\n",
        "\n",
        "    print(f\"Class accuracy is: {(correct_class/(tot_class_preds+1e-16))*100:2f}%\")\n",
        "    print(f\"No obj accuracy is: {(correct_noobj/(tot_noobj+1e-16))*100:2f}%\")\n",
        "    print(f\"Obj accuracy is: {(correct_obj/(tot_obj+1e-16))*100:2f}%\")\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def plot_couple_examples(model, loader, thresh, iou_thresh, anchors, with_targets):\n",
        "    model.eval()\n",
        "    x, y = next(iter(loader))\n",
        "    x = x.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        bboxes = [[] for _ in range(x.shape[0])]\n",
        "        target_bboxes = [[] for _ in range(x.shape[0])]\n",
        "        for i in range(3):\n",
        "            batch_size, A, S, _, _ = out[i].shape\n",
        "            anchor = anchors[i]\n",
        "            boxes_scale_i = cells_to_bboxes(\n",
        "                out[i], anchor, S=S, is_preds=True\n",
        "            )\n",
        "            for idx, (box) in enumerate(boxes_scale_i):\n",
        "                bboxes[idx] += box\n",
        "\n",
        "            if with_targets:\n",
        "                batch_size, A, S, _, _ = y[i].shape\n",
        "                anchor = anchors[i]\n",
        "                boxes_scale_i = cells_to_bboxes(\n",
        "                    y[i], anchor, S=S, is_preds=False\n",
        "                )\n",
        "                for idx, (box) in enumerate(boxes_scale_i):\n",
        "                    target_bboxes[idx] += box\n",
        "\n",
        "        model.train()\n",
        "\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        nms_boxes = non_max_suppression(\n",
        "            bboxes[i], iou_threshold=iou_thresh, threshold=thresh, box_format=\"midpoint\",\n",
        "        )\n",
        "        if with_targets:\n",
        "          nms_target_boxes = non_max_suppression(\n",
        "              target_bboxes[i], iou_threshold=iou_thresh, threshold=thresh, box_format=\"midpoint\",\n",
        "          )\n",
        "          plot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes, nms_target_boxes)\n",
        "        else:\n",
        "          plot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxllPrrwcc3S",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title YOLOv3 Dataset\n",
        "\n",
        "\n",
        "class YOLODataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_list,\n",
        "        label_list,\n",
        "        img_dir,\n",
        "        label_dir,\n",
        "        anchors,\n",
        "        image_size=416,\n",
        "        S=[13, 26, 52],\n",
        "        C=1,\n",
        "        transform=None,\n",
        "    ):\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_size = image_size\n",
        "        self.transform = transform\n",
        "        self.S = S\n",
        "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
        "        self.num_anchors = self.anchors.shape[0]\n",
        "        self.num_anchors_per_scale = self.num_anchors // 3\n",
        "        self.C = C\n",
        "        self.ignore_iou_thresh = 0.5\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.img_dir, self.img_list[index])\n",
        "        img = Image.open(img_path)\n",
        "        image_width, image_height = img.size\n",
        "        image = np.array(img.convert(\"RGB\"))\n",
        "\n",
        "        label_path = os.path.join(self.label_dir, self.label_list[index])\n",
        "\n",
        "         # Read the CSV file\n",
        "        df = pd.read_csv(label_path)\n",
        "\n",
        "        # Assuming 'polyp' is the only class, so class_id = 0\n",
        "        df['class_name'] = 0\n",
        "\n",
        "        # normalize the coordinates of bboxes in the format of YOLO\n",
        "        # Normalize bounding box coordinates\n",
        "        df['x_center'] = (df['xmin'] + df['xmax']) / 2 / image_width\n",
        "        df['y_center'] = (df['ymin'] + df['ymax']) / 2 / image_height\n",
        "        df['width'] = (df['xmax'] - df['xmin']) / image_width\n",
        "        df['height'] = (df['ymax'] - df['ymin']) / image_height\n",
        "        yolo_format = df[['class_name', 'x_center', 'y_center', 'width', 'height']].to_numpy()\n",
        "        bboxes = np.roll(yolo_format, 4, axis=1).tolist()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
        "            image = augmentations[\"image\"]\n",
        "            bboxes = augmentations[\"bboxes\"]\n",
        "\n",
        "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
        "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
        "        for box in bboxes:\n",
        "            iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors)\n",
        "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
        "            x, y, width, height, class_label = box\n",
        "            has_anchor = [False] * 3  # each scale should have one anchor\n",
        "            for anchor_idx in anchor_indices:\n",
        "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
        "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
        "                S = self.S[scale_idx]\n",
        "                i, j = int(S * y), int(S * x)  # which cell\n",
        "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "                if not anchor_taken and not has_anchor[scale_idx]:\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1 # confidence score\n",
        "                    x_cell, y_cell = S * x - j, S * y - i  # both between [0,1]\n",
        "                    width_cell, height_cell = (\n",
        "                        width * S,\n",
        "                        height * S,\n",
        "                    )  # can be greater than 1 since it's relative to cell\n",
        "                    box_coordinates = torch.tensor(\n",
        "                        [x_cell, y_cell, width_cell, height_cell]\n",
        "                    )\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "                    has_anchor[scale_idx] = True\n",
        "\n",
        "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
        "\n",
        "        return image, tuple(targets)\n",
        "\n",
        "# (confidence score, x_center, y_center, width, height, class_label) for each cell of each scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AgXnf8BMdviE"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Test\n",
        "def test():\n",
        "    anchors = ANCHORS\n",
        "\n",
        "    transform = test_transforms\n",
        "\n",
        "    dataset = YOLODataset(\n",
        "        val_images_list,\n",
        "        val_labels_list,\n",
        "        IMG_DIR,\n",
        "        LABEL_DIR,\n",
        "        S=[13, 26, 52],\n",
        "        anchors=anchors,\n",
        "        transform=transform,\n",
        "    )\n",
        "    S = [13, 26, 52]\n",
        "    # [3, 3, 2]\n",
        "    scaled_anchors = torch.tensor(anchors) / (\n",
        "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "    )\n",
        "    loader = DataLoader(dataset=dataset, batch_size=10, shuffle=True)\n",
        "    # y:[3, b, 3, S, S, 6]; 6 = [p, box_coordinates, class_label]\n",
        "    for x, y in loader:\n",
        "        boxes = []\n",
        "        print(y[0].shape) # first scale\n",
        "        print('--------')\n",
        "        for i in range(y[0].shape[1]): # 3 diffrent sizes on only first scale\n",
        "            anchor = scaled_anchors[i] # getting the anchors for each scale\n",
        "            print(anchor.shape)\n",
        "            print(y[i].shape)\n",
        "            print(len(cells_to_bboxes(\n",
        "                y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
        "            )[0]))\n",
        "            boxes += cells_to_bboxes(\n",
        "                y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
        "            )[0] # select the first element's boxes from the batch\n",
        "\n",
        "        print('--------')\n",
        "        print(len(boxes))\n",
        "        print('--------')\n",
        "        boxes = non_max_suppression(boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\")\n",
        "        print(boxes)\n",
        "        plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynimnyu4dp7B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title YOLOv3 architecture\n",
        "\"\"\"\n",
        "Implementation of YOLOv3 architecture\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Information about architecture config:\n",
        "Tuple is structured by (filters, kernel_size, stride)\n",
        "Every conv is a same convolution.\n",
        "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
        "\"S\" is for scale prediction block and computing the yolo loss\n",
        "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "config = [\n",
        "    (32, 3, 1),\n",
        "    (64, 3, 2),\n",
        "    [\"B\", 1],\n",
        "    (128, 3, 2),\n",
        "    [\"B\", 2],\n",
        "    (256, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (512, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (1024, 3, 2),\n",
        "    [\"B\", 4],  # To this point is Darknet-53\n",
        "    (512, 1, 1),\n",
        "    (1024, 3, 1),\n",
        "    \"S\",\n",
        "    (256, 1, 1),\n",
        "    \"U\",\n",
        "    (256, 1, 1),\n",
        "    (512, 3, 1),\n",
        "    \"S\",\n",
        "    (128, 1, 1),\n",
        "    \"U\",\n",
        "    (128, 1, 1),\n",
        "    (256, 3, 1),\n",
        "    \"S\",\n",
        "]\n",
        "\n",
        "\n",
        "# Batch norm and leaky relu added to it\n",
        "# out_channels: Number of filters applied by the convolutional layer, which equals the number of output channels\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
        "        # Calls the constructor of the parent class nn.Module\n",
        "        super().__init__()\n",
        "        # **kwargs allows passing additional parameters like kernel_size, stride, padding, etc\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
        "        # Initializes a batch normalization layer for the output channels of the convolutional layer\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.1)\n",
        "        self.use_bn_act = bn_act\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_bn_act:\n",
        "            return self.leaky(self.bn(self.conv(x)))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "# A combination of two convolutional blocks with a residual connection\n",
        "# The input size will therefore be maintained through the residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for repeat in range(num_repeats):\n",
        "            self.layers += [\n",
        "                nn.Sequential(\n",
        "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
        "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
        "                )\n",
        "            ]\n",
        "        self.use_residual = use_residual\n",
        "        self.num_repeats = num_repeats\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            if self.use_residual:\n",
        "                x = x + layer(x)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# The last two convolutional layers leading up to the prediction for each scale\n",
        "# reshape the output such that it has the the shape (batch size, anchors per scale, grid size, grid size, 5 + number of classes)\n",
        "# where 5 refers to the object score and four bounding box coordinates\n",
        "class ScalePrediction(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.pred = nn.Sequential(\n",
        "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
        "            CNNBlock(\n",
        "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
        "            ),\n",
        "        )\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (\n",
        "            self.pred(x)\n",
        "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
        "            .permute(0, 1, 3, 4, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "class YOLOv3(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=80):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.in_channels = in_channels\n",
        "        self.layers = self._create_conv_layers()\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []  # for each scale\n",
        "        route_connections = []\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, ScalePrediction):\n",
        "                outputs.append(layer(x))\n",
        "                continue\n",
        "\n",
        "            x = layer(x)\n",
        "\n",
        "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
        "                route_connections.append(x)\n",
        "\n",
        "            elif isinstance(layer, nn.Upsample):\n",
        "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
        "                route_connections.pop()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _create_conv_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for module in config:\n",
        "            if isinstance(module, tuple):\n",
        "                out_channels, kernel_size, stride = module\n",
        "                layers.append(\n",
        "                    CNNBlock(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        kernel_size=kernel_size,\n",
        "                        stride=stride,\n",
        "                        padding=1 if kernel_size == 3 else 0,\n",
        "                    )\n",
        "                )\n",
        "                in_channels = out_channels\n",
        "\n",
        "            elif isinstance(module, list):\n",
        "                num_repeats = module[1]\n",
        "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
        "\n",
        "            elif isinstance(module, str):\n",
        "                if module == \"S\":\n",
        "                    layers += [\n",
        "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
        "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
        "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
        "                    ]\n",
        "                    in_channels = in_channels // 2\n",
        "\n",
        "                elif module == \"U\":\n",
        "                    layers.append(nn.Upsample(scale_factor=2),)\n",
        "                    in_channels = in_channels * 3\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_classes = 1\n",
        "    IMAGE_SIZE = 416\n",
        "    model = YOLOv3(num_classes=num_classes)\n",
        "    x = torch.randn((2, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
        "    out = model(x)\n",
        "    assert model(x)[0].shape == (2, 3, IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes + 5)\n",
        "    assert model(x)[1].shape == (2, 3, IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes + 5)\n",
        "    assert model(x)[2].shape == (2, 3, IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes + 5)\n",
        "    print(\"Success!\")\n",
        "    print(\"input shape:\", x.shape)\n",
        "    print(\"output shape 1:\", out[0].shape)\n",
        "    print(\"output shape 2:\", out[1].shape)\n",
        "    print(\"output shape 3:\", out[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uDeDD_-lcoWu"
      },
      "outputs": [],
      "source": [
        "# @title mAP\n",
        "def get_evaluation_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    anchors,\n",
        "    threshold,\n",
        "    box_format=\"midpoint\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    # make sure model is in eval before get bboxes\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "    for batch_idx, (x, labels) in enumerate(tqdm(loader)):\n",
        "        x = x.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        bboxes = [[] for _ in range(batch_size)]\n",
        "        for i in range(3):\n",
        "            S = predictions[i].shape[2]\n",
        "            anchor = torch.tensor([*anchors[i]]).to(device) * S\n",
        "            boxes_scale_i = cells_to_bboxes(\n",
        "                predictions[i], anchor, S=S, is_preds=True\n",
        "            )\n",
        "            for idx, (box) in enumerate(boxes_scale_i):\n",
        "                bboxes[idx] += box\n",
        "\n",
        "        # we just want one bbox for each label, not one for each scale\n",
        "        true_bboxes = cells_to_bboxes(\n",
        "            labels[2], anchor, S=S, is_preds=False\n",
        "        )\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "\n",
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Video explanation of this function:\n",
        "    https://youtu.be/FppOzcDvaDI\n",
        "\n",
        "    This function calculates mean average precision (mAP)\n",
        "\n",
        "    Parameters:\n",
        "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
        "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "        num_classes (int): number of classes\n",
        "\n",
        "    Returns:\n",
        "        float: mAP value across all classes given a specific IoU threshold\n",
        "    \"\"\"\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        # Go through all predictions and targets,\n",
        "        # and only add the ones that belong to the\n",
        "        # current class c\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # find the amount of bboxes for each training example\n",
        "        # Counter here finds how many ground truth bboxes we get\n",
        "        # for each training example, so let's say img 0 has 3,\n",
        "        # img 1 has 5 then we will obtain a dictionary with:\n",
        "        # amount_bboxes = {0:3, 1:5}\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # We then go through each key, val in this dictionary\n",
        "        # and convert to the following (w.r.t same example):\n",
        "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "\n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only take out the ground_truths that have the same\n",
        "            # training idx as detection\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title F1\n",
        "def mean_average_precision_with_F1(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Video explanation of this function:\n",
        "    https://youtu.be/FppOzcDvaDI\n",
        "\n",
        "    This function calculates mean average precision (mAP)\n",
        "\n",
        "    Parameters:\n",
        "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
        "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "        num_classes (int): number of classes\n",
        "\n",
        "    Returns:\n",
        "        float: mAP value across all classes given a specific IoU threshold\n",
        "    \"\"\"\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        # Go through all predictions and targets,\n",
        "        # and only add the ones that belong to the\n",
        "        # current class c\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # find the amount of bboxes for each training example\n",
        "        # Counter here finds how many ground truth bboxes we get\n",
        "        # for each training example, so let's say img 0 has 3,\n",
        "        # img 1 has 5 then we will obtain a dictionary with:\n",
        "        # amount_bboxes = {0:3, 1:5}\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # We then go through each key, val in this dictionary\n",
        "        # and convert to the following (w.r.t same example):\n",
        "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "\n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only take out the ground_truths that have the same\n",
        "            # training idx as detection\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "        # F1 Score\n",
        "        f1_score = (2 * (precisions *recalls)) / (precisions + recalls)\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions), f1_score"
      ],
      "metadata": {
        "id": "MCsKhPwuctyR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rf1UD0mXc_mO"
      },
      "outputs": [],
      "source": [
        "# @title YOLOv3 Loss function\n",
        "\"\"\"\n",
        "Implementation of Yolo Loss Function similar to the one in Yolov3 paper,\n",
        "the difference from what I can tell is I use CrossEntropy for the classes\n",
        "instead of BinaryCrossEntropy.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "YOLO (v3) architecture was optimized on a combination of four losses:\n",
        "no object loss, object loss, box coordinate loss, and class loss.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        # sigmoid function and then binary crossentropy loss\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.entropy = nn.CrossEntropyLoss()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Constants signifying how much to pay for each respective part of the loss\n",
        "        self.lambda_class = 1\n",
        "        self.lambda_noobj = 5\n",
        "        self.lambda_obj = 1\n",
        "        self.lambda_box = 5\n",
        "\n",
        "    def forward(self, predictions, target, anchors):\n",
        "        # Check where obj and noobj (we ignore if target == -1)\n",
        "        obj = target[..., 0] == 1  # in paper this is Iobj_i\n",
        "        noobj = target[..., 0] == 0  # in paper this is Inoobj_i\n",
        "\n",
        "        # ======================= #\n",
        "        #   FOR NO OBJECT LOSS    #\n",
        "        # ======================= #\n",
        "        \"\"\"\n",
        "        We want to incur loss only for their object score.\n",
        "        The target will be all zeros since we want these\n",
        "        anchors to predict an object score of zero\n",
        "\n",
        "        a sigmoid function to the network outputs\n",
        "        \"\"\"\n",
        "        no_object_loss = self.bce(\n",
        "            (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
        "        )\n",
        "\n",
        "        # ==================== #\n",
        "        #   FOR OBJECT LOSS    #\n",
        "        # ==================== #\n",
        "\n",
        "        \"\"\"\n",
        "        the loss will only be applied to the anchors assigned to a target bb\n",
        "        signified by indexing by obj.\n",
        "\n",
        "        \"\"\"\n",
        "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
        "        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
        "        ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
        "        object_loss = self.mse(self.sigmoid(predictions[..., 0:1][obj]), ious * target[..., 0:1][obj])\n",
        "\n",
        "        # ======================== #\n",
        "        #   FOR BOX COORDINATES    #\n",
        "        # ======================== #\n",
        "\n",
        "        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])  # x,y coordinates\n",
        "        target[..., 3:5] = torch.log(\n",
        "            (1e-16 + target[..., 3:5] / anchors)\n",
        "        )  # width, height coordinates\n",
        "        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
        "\n",
        "        # ================== #\n",
        "        #   FOR CLASS LOSS   #\n",
        "        # ================== #\n",
        "\n",
        "        class_loss = self.entropy(\n",
        "            (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
        "        )\n",
        "\n",
        "        #print(\"__________________________________\")\n",
        "        #print(self.lambda_box * box_loss)\n",
        "        #print(self.lambda_obj * object_loss)\n",
        "        #print(self.lambda_noobj * no_object_loss)\n",
        "        #print(self.lambda_class * class_loss)\n",
        "        #print(\"\\n\")\n",
        "\n",
        "        return (\n",
        "            self.lambda_box * box_loss\n",
        "            + self.lambda_obj * object_loss\n",
        "            + self.lambda_noobj * no_object_loss\n",
        "            + self.lambda_class * class_loss\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MnNVp7ywbt_u"
      },
      "outputs": [],
      "source": [
        "# @title Utils for Training\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, current_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = current_loss\n",
        "        elif self.best_loss - current_loss > self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
        "    model.train()  # Ensure model is in training mode\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    losses = []\n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        x = x.to(DEVICE)\n",
        "        y0, y1, y2 = (\n",
        "            y[0].to(DEVICE),\n",
        "            y[1].to(DEVICE),\n",
        "            y[2].to(DEVICE),\n",
        "        )\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(x)\n",
        "            loss = (\n",
        "                loss_fn(out[0], y0, scaled_anchors[0])\n",
        "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "            )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update progress bar\n",
        "        mean_loss = sum(losses) / len(losses)\n",
        "        loop.set_postfix(loss=mean_loss)\n",
        "\n",
        "    return mean_loss  # Return the mean loss for this epoch\n",
        "\n",
        "def validate_fn(val_loader, model, loss_fn, scaled_anchors):\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    val_losses = []\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        loop = tqdm(val_loader, leave=True)\n",
        "        for batch_idx, (x, y) in enumerate(loop):\n",
        "            x = x.to(DEVICE)\n",
        "            y0, y1, y2 = y[0].to(DEVICE), y[1].to(DEVICE), y[2].to(DEVICE)\n",
        "\n",
        "            out = model(x)\n",
        "            loss = (loss_fn(out[0], y0, scaled_anchors[0]) +\n",
        "                    loss_fn(out[1], y1, scaled_anchors[1]) +\n",
        "                    loss_fn(out[2], y2, scaled_anchors[2]))\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "            mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "            loop.set_postfix(val_loss=mean_val_loss)\n",
        "\n",
        "    return mean_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9EXNMG5EDar",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Train\n",
        "\"\"\"\n",
        "Main file for training Yolo model on Pascal VOC and COCO dataset\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "print(ANCHORS)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, current_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = current_loss\n",
        "        elif self.best_loss - current_loss > self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
        "    model.train()  # Ensure model is in training mode\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    losses = []\n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        x = x.to(DEVICE)\n",
        "        y0, y1, y2 = (\n",
        "            y[0].to(DEVICE),\n",
        "            y[1].to(DEVICE),\n",
        "            y[2].to(DEVICE),\n",
        "        )\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(x)\n",
        "            loss = (\n",
        "                loss_fn(out[0], y0, scaled_anchors[0])\n",
        "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "            )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update progress bar\n",
        "        mean_loss = sum(losses) / len(losses)\n",
        "        loop.set_postfix(loss=mean_loss)\n",
        "\n",
        "    return mean_loss  # Return the mean loss for this epoch\n",
        "\n",
        "def validate_fn(val_loader, model, loss_fn, scaled_anchors):\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    val_losses = []\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        loop = tqdm(val_loader, leave=True)\n",
        "        for batch_idx, (x, y) in enumerate(loop):\n",
        "            x = x.to(DEVICE)\n",
        "            y0, y1, y2 = y[0].to(DEVICE), y[1].to(DEVICE), y[2].to(DEVICE)\n",
        "\n",
        "            out = model(x)\n",
        "            loss = (loss_fn(out[0], y0, scaled_anchors[0]) +\n",
        "                    loss_fn(out[1], y1, scaled_anchors[1]) +\n",
        "                    loss_fn(out[2], y2, scaled_anchors[2]))\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "            mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "            loop.set_postfix(val_loss=mean_val_loss)\n",
        "\n",
        "    return mean_val_loss\n",
        "\n",
        "def main():\n",
        "    model = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "    # parameter counts\n",
        "    # Total parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # Trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"Total Parameters: {total_params}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params}\")\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    loss_fn = YoloLoss()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    train_loader, test_loader, train_eval_loader = get_loaders(\n",
        "        train_images_list, train_labels_list, val_images_list, val_labels_list\n",
        "    )\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_FILE, model, optimizer, LEARNING_RATE\n",
        "        )\n",
        "\n",
        "    scaled_anchors = (\n",
        "        torch.tensor(ANCHORS)\n",
        "        * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    epoch_losses = []  # List to store loss per epoch\n",
        "    val_losses = []\n",
        "    early_stopping = EarlyStopping(patience=5, min_delta=0.01)  # Set patience and min_delta here\n",
        "\n",
        "    epoch = 0\n",
        "\n",
        "    while True:\n",
        "        #plot_couple_examples(model, test_loader, 0.6, 0.5, scaled_anchors)\n",
        "        print(f\"Currently epoch {epoch}\")\n",
        "        # training step\n",
        "        mean_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
        "        epoch_losses.append(mean_loss)  # Store the loss for this epoch\n",
        "\n",
        "        # Validation step\n",
        "        mean_val_loss = validate_fn(train_eval_loader, model, loss_fn, scaled_anchors)\n",
        "        val_losses.append(mean_val_loss)\n",
        "\n",
        "        # Early Stopping logic\n",
        "        early_stopping(mean_val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "        # if (epoch + 1)  % 10 == 0 and epoch > 0:\n",
        "\n",
        "        #    if SAVE_MODEL:\n",
        "        #       save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
        "\n",
        "        #    check_class_accuracy(model, train_loader, threshold=CONF_THRESHOLD)\n",
        "\n",
        "          #  pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "          #       train_loader,\n",
        "          #       model,\n",
        "          #       iou_threshold=NMS_IOU_THRESH,\n",
        "          #       anchors=ANCHORS,\n",
        "          #       threshold=CONF_THRESHOLD,\n",
        "          #   )\n",
        "          #  mapval = mean_average_precision(\n",
        "          #       pred_boxes,\n",
        "          #       true_boxes,\n",
        "          #       iou_threshold=MAP_IOU_THRESH,\n",
        "          #       box_format=\"midpoint\",\n",
        "          #       num_classes=NUM_CLASSES,\n",
        "          #   )\n",
        "          #  print(f\"MAP: {mapval.item()}\")\n",
        "          #  model.train()\n",
        "\n",
        "        #print(f\"Currently epoch {epoch}\")\n",
        "        #print(\"On Train Eval loader:\")\n",
        "        #print(\"On Train loader:\")\n",
        "        # check_class_accuracy(model, train_loader, threshold=CONF_THRESHOLD)\n",
        "\n",
        "        # if epoch > 0 and epoch % 3 == 0:\n",
        "        #     check_class_accuracy(model, test_loader, threshold=CONF_THRESHOLD)\n",
        "            # pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "            #     test_loader,\n",
        "            #     model,\n",
        "            #     iou_threshold=NMS_IOU_THRESH,\n",
        "            #     anchors=ANCHORS,\n",
        "            #     threshold=CONF_THRESHOLD,\n",
        "            # )\n",
        "            # mapval = mean_average_precision(\n",
        "            #     pred_boxes,\n",
        "            #     true_boxes,\n",
        "            #     iou_threshold=MAP_IOU_THRESH,\n",
        "            #     box_format=\"midpoint\",\n",
        "            #     num_classes=NUM_CLASSES,\n",
        "            # )\n",
        "            # print(f\"MAP: {mapval.item()}\")\n",
        "            # model.train()\n",
        "        epoch += 1\n",
        "\n",
        "    # save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
        "    torch.save(model, 'second_model_complete.pth')\n",
        "    # Plot the loss after training\n",
        "    plt.plot(epoch_losses, label=\"Training Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss Over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PDdVKz2qEMWQ"
      },
      "outputs": [],
      "source": [
        "# @title Test using loader\n",
        "\n",
        "model = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "loss_fn = YoloLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "train_loader, test_loader, train_eval_loader = get_loaders(\n",
        "        train_images_list, train_labels_list, val_images_list, val_labels_list\n",
        ")\n",
        "\n",
        "if True:\n",
        "        load_checkpoint(\n",
        "            \"/content/checkpoint.pth.tar\", model, optimizer, LEARNING_RATE\n",
        ")\n",
        "\n",
        "scaled_anchors = (\n",
        "        torch.tensor(ANCHORS)\n",
        "        * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        ").to(DEVICE)\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "validate_fn(train_eval_loader, model, loss_fn, scaled_anchors)\n",
        "\n",
        "plot_couple_examples(model, test_loader, 0.6, 0.45, scaled_anchors, with_targets=True)\n",
        "\n",
        "\n",
        "\n",
        "print(\"--------------\")\n",
        "pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "                train_loader,\n",
        "                model,\n",
        "                iou_threshold=NMS_IOU_THRESH,\n",
        "                anchors=ANCHORS,\n",
        "                threshold=CONF_THRESHOLD,\n",
        "            )\n",
        "mapval = mean_average_precision(\n",
        "                pred_boxes,\n",
        "                true_boxes,\n",
        "                iou_threshold=MAP_IOU_THRESH,\n",
        "                box_format=\"midpoint\",\n",
        "                num_classes=NUM_CLASSES,\n",
        "            )\n",
        "print(f\"MAP: {mapval.item()}\")\n",
        "\n",
        "check_class_accuracy(model, test_loader, threshold=CONF_THRESHOLD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5xQ1_-V-hioh"
      },
      "outputs": [],
      "source": [
        "# @title Show image two\n",
        "def show_images_two(img_lst, label_lst, loops=8):\n",
        "    for i in range(loops):\n",
        "        img_path = os.path.join(IMG_DIR, img_lst[i])\n",
        "        label_path = os.path.join(LABEL_DIR, label_lst[i])\n",
        "        img = Image.open(img_path)\n",
        "        image_width, image_height = img.size\n",
        "\n",
        "        print(img_path)\n",
        "        # print(img.size)\n",
        "        # print(type(img))\n",
        "        # plt.imshow(img)\n",
        "        # plt.show()\n",
        "\n",
        "\n",
        "        print(label_path)\n",
        "\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(label_path)\n",
        "\n",
        "        # Display the dataframe\n",
        "        print(df)\n",
        "\n",
        "        bounding_boxes = df[['class_name', 'xmin', 'ymin', 'xmax', 'ymax']].to_numpy()\n",
        "        print(bounding_boxes)\n",
        "        print(bounding_boxes.shape)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        ax.imshow(img)\n",
        "        # Plot each bounding box\n",
        "        for index, row in df.iterrows():\n",
        "            x_min, y_min, x_max, y_max = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "            width = x_max - x_min\n",
        "            height = y_max - y_min\n",
        "            # Create a rectangle patch\n",
        "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            # Add the patch to the axis\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # # Assuming 'polyp' is the only class, so class_id = 1\n",
        "        # df['class_name'] = 1\n",
        "\n",
        "        # # Normalize bounding box coordinates\n",
        "        # df['x_center'] = (df['xmin'] + df['xmax']) / 2 / image_width\n",
        "        # df['y_center'] = (df['ymin'] + df['ymax']) / 2 / image_height\n",
        "        # df['width'] = (df['xmax'] - df['xmin']) / image_width\n",
        "        # df['height'] = (df['ymax'] - df['ymin']) / image_height\n",
        "\n",
        "        # # Prepare YOLO format data\n",
        "        # yolo_format = df[['class_name', 'x_center', 'y_center', 'width', 'height']].to_numpy()\n",
        "\n",
        "        # # Display the dataframe\n",
        "        # print(df)\n",
        "\n",
        "        # print(yolo_format)\n",
        "\n",
        "        # print(yolo_format.shape)\n",
        "\n",
        "        # fig, ax = plt.subplots()\n",
        "        # ax.imshow(img)\n",
        "        # # Create a Rectangle patch\n",
        "        # for box in yolo_format:\n",
        "        #     box = box[1:]\n",
        "        #     upper_left_x = box[0] - box[2] / 2\n",
        "        #     upper_left_y = box[1] - box[3] / 2\n",
        "        #     rect = patches.Rectangle(\n",
        "        #         (upper_left_x * image_width, upper_left_y * image_height),\n",
        "        #         box[2] * image_width,\n",
        "        #         box[3] * image_height,\n",
        "        #         linewidth=2,\n",
        "        #         edgecolor='r',\n",
        "        #         facecolor=\"none\",\n",
        "        #     )\n",
        "        #     # Add the patch to the Axes\n",
        "        #     ax.add_patch(rect)\n",
        "\n",
        "\n",
        "        # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(\"----------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMmTQHHecIPv"
      },
      "outputs": [],
      "source": [
        "def find_csv_files_with_more_than_2_rows(directory_path):\n",
        "    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
        "    files_with_more_than_2_rows = []\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        file_path = os.path.join(directory_path, csv_file)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            if len(df) > 2:  # Check if the number of rows is more than 2\n",
        "                # Remove the '.csv' extension\n",
        "                file_name_without_extension = os.path.splitext(csv_file)[0]\n",
        "                files_with_more_than_2_rows.append(csv_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read {csv_file}: {e}\")\n",
        "\n",
        "    return files_with_more_than_2_rows\n",
        "\n",
        "directory_path = LABEL_DIR  # Replace with your directory path\n",
        "files_labels = find_csv_files_with_more_than_2_rows(directory_path)\n",
        "print(len(files_labels))\n",
        "print(\"CSV files with more than 2 rows:\")\n",
        "for file in files_labels:\n",
        "    print(file)\n",
        "\n",
        "\n",
        "files_images = [x.replace(\".csv\", \".jpg\") for x in files_labels]\n",
        "print(\"jpg files with more than 2 rows:\")\n",
        "for file in files_images:\n",
        "    print(file)\n",
        "\n",
        "show_images_two(files_images, files_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bcac8_iHcTRr"
      },
      "outputs": [],
      "source": [
        "# model = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "# optimizer = optim.Adam(\n",
        "#         model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "#     )\n",
        "loss_fn = YoloLoss()\n",
        "# scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# train_loader, test_loader, train_eval_loader = get_loaders(\n",
        "#         files_images, files_labels, files_images, files_labels\n",
        "#     )\n",
        "\n",
        "# if True:\n",
        "#         load_checkpoint(\n",
        "#             CHECKPOINT_FILE, model, optimizer, LEARNING_RATE\n",
        "#         )\n",
        "\n",
        "scaled_anchors = (\n",
        "        torch.tensor(ANCHORS)\n",
        "        * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "    ).to(DEVICE)\n",
        "model = torch.load('/content/drive/MyDrive/models/second_pruned_resnet.pth')\n",
        "model = model.to(DEVICE)\n",
        "# model = model.to(DEVICE)\n",
        "# CONF_THRESHOLD = 0.6\n",
        "# MAP_IOU_THRESH = 0.5\n",
        "# NMS_IOU_THRESH = 0.45\n",
        "\n",
        "train_loader, test_loader, train_eval_loader = get_loaders(\n",
        "    train_images_list, train_labels_list, val_images_list, val_labels_list\n",
        ")\n",
        "plot_couple_examples(model, test_loader, CONF_THRESHOLD, NMS_IOU_THRESH, scaled_anchors, with_targets=True)\n",
        "\n",
        "\n",
        "# Total parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "# Trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")\n",
        "print(\"--------------\")\n",
        "pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "                test_loader,\n",
        "                model,\n",
        "                iou_threshold=NMS_IOU_THRESH,\n",
        "                anchors=ANCHORS,\n",
        "                threshold=CONF_THRESHOLD,\n",
        "            )\n",
        "mapval = mean_average_precision(\n",
        "                pred_boxes,\n",
        "                true_boxes,\n",
        "                iou_threshold=MAP_IOU_THRESH,\n",
        "                box_format=\"midpoint\",\n",
        "                num_classes=NUM_CLASSES,\n",
        "            )\n",
        "print(f\"MAP: {mapval.item()}\")\n",
        "\n",
        "_, f1_score = mean_average_precision_with_F1(\n",
        "                pred_boxes,\n",
        "                true_boxes,\n",
        "                iou_threshold=MAP_IOU_THRESH,\n",
        "                box_format=\"midpoint\",\n",
        "                num_classes=NUM_CLASSES,\n",
        "            )\n",
        "print(f\"F1: {f1_score.item()}\")\n",
        "\n",
        "initial_loss = validate_fn(train_eval_loader, model, loss_fn, scaled_anchors)\n",
        "print(f\"Initial validation loss: {initial_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S_4mv-gr9Xm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8ZvobKEPnfu9"
      },
      "outputs": [],
      "source": [
        "# @title Graph: LOSS PER EPOCH\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "# Path to your tensorboard log directory (change accordingly)\n",
        "log_dir = '/content/lightning_logs/resnet_pruning/version_0/'\n",
        "\n",
        "# Load TensorBoard logs\n",
        "event_acc = EventAccumulator(log_dir)\n",
        "event_acc.Reload()\n",
        "\n",
        "# Retrieve the training and validation losses\n",
        "train_loss = event_acc.Scalars('train_loss')  # Use the name of the scalar you logged (e.g., 'train_loss')\n",
        "val_loss = event_acc.Scalars('val_loss')  # If you logged validation loss as well\n",
        "\n",
        "# Extract the steps (epochs) and loss values\n",
        "train_steps = [x.step for x in train_loss]\n",
        "train_losses = [x.value for x in train_loss]\n",
        "\n",
        "# Optional: If you logged validation losses\n",
        "val_steps = [x.step for x in val_loss]\n",
        "val_losses = [x.value for x in val_loss]\n",
        "\n",
        "# Plotting training loss\n",
        "plt.plot(train_steps, train_losses, label='Train Loss')\n",
        "\n",
        "# Plotting validation loss (if available)\n",
        "if val_loss:\n",
        "    plt.plot(val_steps, val_losses, label='Validation Loss')\n",
        "\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss per Step')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jCXgL2c_rF8x"
      },
      "outputs": [],
      "source": [
        " # @title Test using loader\n",
        "model.to(DEVICE)\n",
        "plot_couple_examples(model, test_loader, 0.6, 0.45, scaled_anchors, with_targets=True)\n",
        "\n",
        "# Trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")\n",
        "\n",
        "print(\"--------------\")\n",
        "pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "                train_loader,\n",
        "                model,\n",
        "                iou_threshold=NMS_IOU_THRESH,\n",
        "                anchors=ANCHORS,\n",
        "                threshold=CONF_THRESHOLD,\n",
        "            )\n",
        "mapval = mean_average_precision(\n",
        "                pred_boxes,\n",
        "                true_boxes,\n",
        "                iou_threshold=MAP_IOU_THRESH,\n",
        "                box_format=\"midpoint\",\n",
        "                num_classes=NUM_CLASSES,\n",
        "            )\n",
        "print(f\"MAP: {mapval.item()}\")\n",
        "\n",
        "check_class_accuracy(model, test_loader, threshold=CONF_THRESHOLD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qnhPSzl3S55s"
      },
      "outputs": [],
      "source": [
        "# @title Utils for Training\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, current_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = current_loss\n",
        "        elif self.best_loss - current_loss > self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
        "    model.train()  # Ensure model is in training mode\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    losses = []\n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        x = x.to(DEVICE)\n",
        "        y0, y1, y2 = (\n",
        "            y[0].to(DEVICE),\n",
        "            y[1].to(DEVICE),\n",
        "            y[2].to(DEVICE),\n",
        "        )\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(x)\n",
        "            loss = (\n",
        "                loss_fn(out[0], y0, scaled_anchors[0])\n",
        "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "            )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update progress bar\n",
        "        mean_loss = sum(losses) / len(losses)\n",
        "        loop.set_postfix(loss=mean_loss)\n",
        "\n",
        "    return mean_loss  # Return the mean loss for this epoch\n",
        "\n",
        "def validate_fn(val_loader, model, loss_fn, scaled_anchors):\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    val_losses = []\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        loop = tqdm(val_loader, leave=True)\n",
        "        for batch_idx, (x, y) in enumerate(loop):\n",
        "            x = x.to(DEVICE)\n",
        "            y0, y1, y2 = y[0].to(DEVICE), y[1].to(DEVICE), y[2].to(DEVICE)\n",
        "\n",
        "            out = model(x)\n",
        "            loss = (loss_fn(out[0], y0, scaled_anchors[0]) +\n",
        "                    loss_fn(out[1], y1, scaled_anchors[1]) +\n",
        "                    loss_fn(out[2], y2, scaled_anchors[2]))\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "            mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "            loop.set_postfix(val_loss=mean_val_loss)\n",
        "\n",
        "    return mean_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZJ47xM4v7Rkm"
      },
      "outputs": [],
      "source": [
        "# @title Taylor Pruning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def taylor_prune_conv_layer(layer, model, loss_fn, validloader, device, scaled_anchors, num_filters_to_prune):\n",
        "    # Initialize variables\n",
        "    filter_ranks = []\n",
        "    activations = []\n",
        "    grad_index = 0\n",
        "\n",
        "    # Register forward hook to capture activations\n",
        "    def capture_activations(module, input, output):\n",
        "        activations.append(output.detach())\n",
        "\n",
        "    # Register backward hook to compute rank\n",
        "    def compute_rank(grad):\n",
        "        nonlocal grad_index\n",
        "        act_idx = len(activations) - grad_index - 1\n",
        "        # Compute rank: [B, C, H, W] -> [B, C]\n",
        "        rank = torch.abs(torch.mean((grad * activations[act_idx]), dim=(2, 3)))\n",
        "        # Normalize each filter rank by L2 norm (across batches)\n",
        "        div = torch.sqrt(torch.sum(rank ** 2, dim=1, keepdim=True))\n",
        "        rank = rank / div\n",
        "        # Mean over dimension 0 (batch size): [C]\n",
        "        rank = torch.mean(rank, dim=0)\n",
        "\n",
        "        if len(filter_ranks) < grad_index + 1:\n",
        "            filter_ranks.append(rank)\n",
        "        else:\n",
        "            filter_ranks[grad_index] += rank\n",
        "\n",
        "        grad_index += 1\n",
        "\n",
        "    # Register hooks\n",
        "    handle_forward = layer.register_forward_hook(capture_activations)\n",
        "    handle_backward = layer.register_backward_hook(lambda module, grad_input, grad_output: compute_rank(grad_output[0]))\n",
        "\n",
        "    # Iterate through validation data\n",
        "    model.eval()\n",
        "    # for batch in validloader:\n",
        "    #     x = batch[0].to(device)\n",
        "    #     labels = batch[1].to(device)\n",
        "\n",
        "    #     # Forward pass\n",
        "    #     output = model(x)\n",
        "\n",
        "    #     # Backward pass\n",
        "    #     loss = nn.functional.cross_entropy(output, labels)\n",
        "    #     model.zero_grad()  # Clear previous gradients\n",
        "    #     loss.backward()\n",
        "\n",
        "    #     # Clear activations for next batch\n",
        "    #     activations.clear()\n",
        "    #     grad_index = 0\n",
        "    for batch_idx, (x, y) in enumerate(tqdm(validloader, leave=True)):\n",
        "        x = x.to(device)\n",
        "        y0, y1, y2 = y[0].to(device), y[1].to(device), y[2].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(x)\n",
        "        loss = (loss_fn(out[0], y0, scaled_anchors[0]) +\n",
        "                loss_fn(out[1], y1, scaled_anchors[1]) +\n",
        "                loss_fn(out[2], y2, scaled_anchors[2]))\n",
        "\n",
        "        # Backward pass: Required for Taylor pruning to get gradients\n",
        "        model.zero_grad()  # Clear previous gradients\n",
        "        loss.backward()  # Compute gradients\n",
        "\n",
        "        # val_losses.append(loss.item())\n",
        "        # mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "\n",
        "        # Clear activations for next batch\n",
        "        activations.clear()\n",
        "        grad_index = 0\n",
        "\n",
        "    # Remove hooks\n",
        "    handle_forward.remove()\n",
        "    handle_backward.remove()\n",
        "\n",
        "    # Compute final ranks\n",
        "    if filter_ranks:\n",
        "        final_ranks = torch.stack(filter_ranks).sum(dim=0)\n",
        "    else:\n",
        "        raise ValueError(\"No filter ranks computed. Check if the layer is part of the model's forward pass.\")\n",
        "\n",
        "    # Get indices of filters to prune\n",
        "    sorted_indices = torch.argsort(final_ranks)\n",
        "    filters_to_prune = sorted_indices[:num_filters_to_prune]\n",
        "\n",
        "    # Create mask\n",
        "    mask = torch.ones(layer.out_channels, dtype=torch.bool, device=device)\n",
        "    mask[filters_to_prune] = False\n",
        "\n",
        "    # Create new layer\n",
        "    new_out_channels = mask.sum().item()\n",
        "    new_layer = nn.Conv2d(layer.in_channels, new_out_channels,\n",
        "                          kernel_size=layer.kernel_size, stride=layer.stride,\n",
        "                          padding=layer.padding, bias=(layer.bias is not None)).to(device)\n",
        "\n",
        "    # Copy weights and bias of kept filters\n",
        "    new_layer.weight.data = layer.weight.data[mask].clone()\n",
        "    if layer.bias is not None:\n",
        "        new_layer.bias.data = layer.bias.data[mask].clone()\n",
        "\n",
        "    return new_layer, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oCiPeU-kMISu"
      },
      "outputs": [],
      "source": [
        "# @title Randomly pruning\n",
        "\n",
        "def random_prune_conv_layer(layer, num_filters_to_prune):\n",
        "    # Get the total number of filters in the layer\n",
        "    total_filters = layer.weight.size(0)\n",
        "\n",
        "    # Randomly select filters to prune\n",
        "    filters_to_prune = torch.randperm(total_filters)[:num_filters_to_prune]\n",
        "\n",
        "    # Create a mask for the filters (True for filters to keep, False for filters to prune)\n",
        "    mask = torch.ones(total_filters, dtype=torch.bool, device=layer.weight.device)\n",
        "    mask[filters_to_prune] = False\n",
        "\n",
        "    # Calculate the number of remaining filters\n",
        "    new_out_channels = mask.sum().item()\n",
        "\n",
        "    # Create a new layer with the reduced number of filters\n",
        "    new_layer = nn.Conv2d(layer.in_channels, new_out_channels,\n",
        "                          kernel_size=layer.kernel_size, stride=layer.stride,\n",
        "                          padding=layer.padding, bias=(layer.bias is not None))\n",
        "\n",
        "    # Copy the weights and bias (if exists) of the kept filters to the new layer\n",
        "    new_layer.weight.data = layer.weight.data[mask].clone()\n",
        "    if layer.bias is not None:\n",
        "        new_layer.bias.data = layer.bias.data[mask].clone()\n",
        "\n",
        "    return new_layer, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvXNcH417zL6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "from dataclasses import replace\n",
        "# @title Utils for Pruning (New Version)\n",
        "\n",
        "def update_lightning_model(lightning_model):\n",
        "\n",
        "    total_params = sum(p.numel() for p in lightning_model.model.parameters())\n",
        "\n",
        "    # Trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in lightning_model.model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"Total Parameters: {total_params}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params}\")\n",
        "\n",
        "\n",
        "    lightning_model = LightningYOLOModule(lightning_model.model, scaled_anchors)\n",
        "    return lightning_model\n",
        "\n",
        "def prune_conv_layer(layer, num_filters_to_prune):\n",
        "    weight = layer.weight.data # [out_channels, in_channels, kernel_size, kernel_size]\n",
        "    l1_norm = weight.abs().sum(dim=(1, 2, 3)) # weights of a fitler; 5 filters total [5]\n",
        "    sorted_indices = torch.argsort(l1_norm)\n",
        "    filters_to_prune = sorted_indices[:num_filters_to_prune]\n",
        "\n",
        "    mask = torch.ones(weight.size(0), dtype=torch.bool)\n",
        "    mask[filters_to_prune] = False\n",
        "\n",
        "    new_out_channels = mask.sum().item()\n",
        "    new_layer = nn.Conv2d(layer.in_channels, new_out_channels,\n",
        "                          kernel_size=layer.kernel_size, stride=layer.stride,\n",
        "                          padding=layer.padding, bias=(layer.bias is not None))\n",
        "\n",
        "    new_layer.weight.data = layer.weight.data[mask].clone()\n",
        "    if layer.bias is not None:\n",
        "        new_layer.bias.data = layer.bias.data[mask].clone()\n",
        "\n",
        "\n",
        "    return new_layer, mask\n",
        "\n",
        "def get_current_layer(model, updated_layer):\n",
        "\n",
        "    for module in model.modules():\n",
        "        if layers_equal(module, updated_layer):\n",
        "            return module\n",
        "\n",
        "    return None\n",
        "\n",
        "def replace_layer_in_model(model, old_layer, new_layer):\n",
        "    for name, module in model.named_modules():\n",
        "        if module is old_layer:\n",
        "            if '.' in name:\n",
        "                parent_name, child_name = name.rsplit('.', 1)\n",
        "                parent = model\n",
        "                for part in parent_name.split('.'):\n",
        "                    parent = getattr(parent, part)\n",
        "                setattr(parent, child_name, new_layer)\n",
        "            else:\n",
        "                setattr(model, name, new_layer)\n",
        "            break\n",
        "\n",
        "def update_batchnorm_for_pruned_conv(bn_layer, filter_indices_to_keep):\n",
        "    \"\"\"\n",
        "    Adjusts the BatchNorm2d layer to match the new output channels of the pruned Conv2d layer.\n",
        "\n",
        "    Parameters:\n",
        "    - bn_layer (nn.BatchNorm2d): The BatchNorm2d layer to be updated.\n",
        "    - filter_indices_to_keep (list): List of indices corresponding to the filters that were kept during pruning.\n",
        "\n",
        "    Returns:\n",
        "    - new_bn (nn.BatchNorm2d): The updated BatchNorm2d layer.\n",
        "    \"\"\"\n",
        "    pruned_out_channels = filter_indices_to_keep.sum().item()\n",
        "    # Create a new BatchNorm2d layer with the pruned number of channels\n",
        "    new_bn = nn.BatchNorm2d(pruned_out_channels)\n",
        "\n",
        "    # Copy only the parameters corresponding to the remaining filters\n",
        "    new_bn.weight.data = bn_layer.weight.data[filter_indices_to_keep]\n",
        "    new_bn.bias.data = bn_layer.bias.data[filter_indices_to_keep]\n",
        "    new_bn.running_mean = bn_layer.running_mean[filter_indices_to_keep]\n",
        "    new_bn.running_var = bn_layer.running_var[filter_indices_to_keep]\n",
        "\n",
        "    return new_bn\n",
        "\n",
        "\n",
        "def layers_are_equal(layer1, layer2):\n",
        "    \"\"\"\n",
        "    Check if two layers are structurally identical by comparing their parameters.\n",
        "\n",
        "    :param layer1: The first layer to compare.\n",
        "    :param layer2: The second layer to compare.\n",
        "    :return: True if the layers are identical, False otherwise.\n",
        "    \"\"\"\n",
        "    if type(layer1) != type(layer2):\n",
        "        return False\n",
        "\n",
        "    for p1, p2 in zip(layer1.parameters(), layer2.parameters()):\n",
        "        if not torch.equal(p1, p2):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# def replace_layer_by_instance(model, old_layer, new_layer, parent_name=\"\"):\n",
        "#     \"\"\"\n",
        "#     Replace CNNBlock, ResidualBlock, and their layers\n",
        "\n",
        "#     :param model: The model or layer containing the layers.\n",
        "#     :param old_layer: The old layer instance to replace.\n",
        "#     :param new_layer: The new layer to insert.\n",
        "#     \"\"\"\n",
        "#     replaced = False\n",
        "\n",
        "#     for name, module in model.layers.named_children():\n",
        "#         full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
        "#         if isinstance(module, CNNBlock)\n",
        "#             for child_name, layer in module.named_children():\n",
        "#                 child_full_name = f\"{full_name}.{child_name}\"\n",
        "#                 if layers_are_equal(layer, old_layer):\n",
        "#                     setattr(model, child_full_name, new_layer)\n",
        "#                     replaced = True\n",
        "#                     break\n",
        "#         elif isinstance(module, ResidualBlock):\n",
        "#             for sequential in module.layers:\n",
        "#                 for block in sequential:\n",
        "#                     for name, layer in block.named_children():\n",
        "#                         if layers_are_equal(layer, old_layer):\n",
        "#                             setattr(model, name, new_layer)\n",
        "#                             replaced = True\n",
        "#                             break\n",
        "\n",
        "#         if replaced:\n",
        "#             break\n",
        "\n",
        "# def replace_layer_by_instance(model, old_layer, new_layer):\n",
        "#     \"\"\"\n",
        "#     Recursively replaces a layer within a nested ModuleList or Sequential model\n",
        "#     by directly comparing the layer instances.\n",
        "\n",
        "#     :param model: The model or layer containing the layers.\n",
        "#     :param old_layer: The old layer instance to replace.\n",
        "#     :param new_layer: The new layer to insert.\n",
        "#     \"\"\"\n",
        "#     for name, layer in model.named_children():\n",
        "#         if layer is old_layer:\n",
        "#             # Direct comparison of the layer instance\n",
        "#             setattr(model, name, new_layer)\n",
        "#             return True  # Early exit when the layer is replaced\n",
        "#         elif isinstance(layer, (nn.ModuleList, nn.Sequential)):\n",
        "#             # Recursively enter nested structures\n",
        "#             if replace_layer_by_instance(layer, old_layer, new_layer):\n",
        "#                 return True  # Early exit when the layer is replaced\n",
        "#     return False  # If no match is found\n",
        "\n",
        "def layers_equal(layer1, layer2):\n",
        "    # Check if layers are of the same type\n",
        "    if type(layer1) != type(layer2):\n",
        "        return False\n",
        "\n",
        "    # Check if layers have the same number of parameters\n",
        "    if sum(p.numel() for p in layer1.parameters()) != sum(p.numel() for p in layer2.parameters()):\n",
        "        return False\n",
        "\n",
        "    # Compare each parameter\n",
        "    for p1, p2 in zip(layer1.parameters(), layer2.parameters()):\n",
        "        p1 = p1.to(p2.device)\n",
        "        if p1.shape != p2.shape:\n",
        "            return False\n",
        "        if not torch.allclose(p1, p2):\n",
        "            return False\n",
        "\n",
        "    # For convolutional layers, check additional attributes\n",
        "    if isinstance(layer1, nn.Conv2d):\n",
        "        attrs_to_check = ['in_channels', 'out_channels', 'kernel_size', 'stride', 'padding', 'dilation', 'groups', 'bias']\n",
        "        for attr in attrs_to_check:\n",
        "            if attr == 'bias':\n",
        "                if getattr(layer1, attr) is not None and getattr(layer2, attr) is not None:\n",
        "                    continue\n",
        "            if getattr(layer1, attr) != getattr(layer2, attr):\n",
        "                return False\n",
        "\n",
        "    # If all checks pass, layers are equal\n",
        "    return True\n",
        "\n",
        "def update_next_layer(layer, mask, model=None, loss_fn=None, train_eval_loader=None, device=None, scaled_anchors=None, prev_model=None):\n",
        "    if isinstance(layer, CNNBlock):\n",
        "        new_layer = update_next_layer(layer.conv, mask, model)\n",
        "        replace_layer_in_model(model, layer.conv, new_layer)\n",
        "        return layer\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        new_in_channels = mask.sum().item()\n",
        "        new_layer = nn.Conv2d(new_in_channels, layer.out_channels,\n",
        "                              kernel_size=layer.kernel_size, stride=layer.stride,\n",
        "                              padding=layer.padding, bias=(layer.bias is not None))\n",
        "\n",
        "        new_layer.weight.data = layer.weight.data[:, mask, :, :].clone()\n",
        "        if layer.bias is not None:\n",
        "            new_layer.bias.data = layer.bias.data.clone()\n",
        "\n",
        "        return new_layer\n",
        "\n",
        "    elif isinstance(layer, ScalePrediction):\n",
        "        first_conv = layer.pred[0].conv\n",
        "        new_in_channels = mask.sum().item()\n",
        "        new_layer = nn.Conv2d(new_in_channels, first_conv.out_channels,\n",
        "                              kernel_size=first_conv.kernel_size, stride=first_conv.stride,\n",
        "                              padding=first_conv.padding, bias=(first_conv.bias is not None))\n",
        "\n",
        "        new_layer.weight.data = first_conv.weight.data[:, mask, :, :].clone()\n",
        "        if first_conv.bias is not None:\n",
        "            new_layer.bias.data = first_conv.bias.data.clone()\n",
        "\n",
        "        layer.pred[0].conv = new_layer\n",
        "        return layer\n",
        "\n",
        "    elif isinstance(layer, nn.Linear):\n",
        "        input_dim = layer.weight.size(1) // len(mask) # nn.Linear.weight == [out_features, in_features]; layer.weight.size(1) == num of in_features\n",
        "        mask = mask.repeat_interleave(input_dim)\n",
        "        new_layer = nn.Linear(mask.sum().item(), layer.out_features)\n",
        "        new_layer.weight.data = layer.weight.data[:, mask].clone()\n",
        "        if layer.bias is not None:\n",
        "            new_layer.bias.data = layer.bias.data.clone()\n",
        "\n",
        "        return new_layer\n",
        "\n",
        "    elif isinstance(layer, ResidualBlock):\n",
        "        block = layer\n",
        "        current_idx = 0\n",
        "        for idx, module in enumerate(model.layers):\n",
        "            if module is layer:\n",
        "                current_idx = idx\n",
        "                break\n",
        "        for idx, sequential in enumerate(block.layers):\n",
        "            # Update the first conv layer of two CNNBlocks\n",
        "            sequential[0].conv = update_next_layer(sequential[0].conv, mask, model)\n",
        "            last_conv = sequential[1].conv\n",
        "            last_bn = sequential[1].bn\n",
        "            # prune the second layer as normal pruning\n",
        "            num_to_prune = int(last_conv.out_channels - mask.sum().item())\n",
        "            if CRITERION == \"Taylor\":\n",
        "                print(\"-----Taylor Pruning-----\")\n",
        "                prev_last_conv = ((prev_model.layers[current_idx]).layers[idx])[1].conv\n",
        "                new_layer, new_mask = taylor_prune_conv_layer(prev_last_conv, prev_model, loss_fn, train_eval_loader, device, scaled_anchors, num_to_prune)\n",
        "\n",
        "            elif CRITERION == \"L1\":\n",
        "                print(\"-----L1 Pruning-----\")\n",
        "                # Update the second conv layer and its bn of two CNNBlocks\n",
        "                new_layer, new_mask = prune_conv_layer(last_conv, num_to_prune)\n",
        "            else: # prune filters randomly\n",
        "                print(\"-----Random Pruning-----\")\n",
        "                # Update the second conv layer and its bn of two CNNBlocks\n",
        "                new_layer, new_mask = random_prune_conv_layer(last_conv, num_to_prune)\n",
        "\n",
        "\n",
        "            # weight = last_conv.weight.data # [out_channels, in_channels, kernel_size, kernel_size]\n",
        "            # l1_norm = weight.abs().sum(dim=(1, 2, 3)) # weights of a fitler; 5 filters total [5]\n",
        "            # sorted_indices = torch.argsort(l1_norm)\n",
        "            # num_to_prune = int(len(sorted_indices) - mask.sum().item())\n",
        "            # filters_to_prune = sorted_indices[:num_to_prune]\n",
        "\n",
        "            # new_mask = torch.ones(weight.size(0), dtype=torch.bool)\n",
        "            # new_mask[filters_to_prune] = False\n",
        "\n",
        "            # new_out_channels = new_mask.sum().item()\n",
        "            # new_layer = nn.Conv2d(last_conv.in_channels, new_out_channels,\n",
        "            #                       kernel_size=last_conv.kernel_size, stride=last_conv.stride,\n",
        "            #                       padding=last_conv.padding, bias=(last_conv.bias is not None))\n",
        "\n",
        "            # new_layer.weight.data = last_conv.weight.data[new_mask].clone()\n",
        "            # if last_conv.bias is not None:\n",
        "            #     new_layer.bias.data = last_conv.bias.data[new_mask].clone()\n",
        "\n",
        "            sequential[1].conv = new_layer\n",
        "            sequential[1].bn = update_batchnorm_for_pruned_conv(last_bn, new_mask)\n",
        "\n",
        "        # Update its next layer and batchNorm (if there is)\n",
        "        next_layer = find_next_module(model, block)\n",
        "        if next_layer is not None:\n",
        "            updated_next_layer = update_next_layer(next_layer, new_mask, model, loss_fn, train_eval_loader, device, scaled_anchors, prev_model)\n",
        "            replace_layer_in_model(model, next_layer, updated_next_layer)\n",
        "\n",
        "        # Update its concat layer input channels if it is 8 residual block\n",
        "        if block is model.layers[6] or block is model.layers[8]:\n",
        "            prune_concat_layer(model, block, new_mask)\n",
        "\n",
        "        return block\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def update_model_after_pruning(model, current_block, idx):\n",
        "    # Update the current block in the model\n",
        "    model.layers[idx] = current_block\n",
        "\n",
        "    # If necessary, update other parts of the model that depend on this block\n",
        "    # This might include updating the input channels of the next block, etc.\n",
        "\n",
        "    # Recreate the model's forward pass if necessary\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def find_next_module(model, current_module):\n",
        "    found_current = False\n",
        "    for layer in model.layers:\n",
        "        if layer is current_module:\n",
        "            found_current = True\n",
        "        elif found_current and isinstance(layer, (CNNBlock, ScalePrediction, ResidualBlock)):\n",
        "            return layer\n",
        "    return None\n",
        "def update_concat_layer(model, block, mask):\n",
        "    if block is model.layers[16]:\n",
        "        layer = model.layers[18].conv\n",
        "        num_in_channels = layer.in_channels\n",
        "        new_mask = torch.ones(num_in_channels, dtype=torch.bool)\n",
        "        new_mask[:len(mask)] = mask\n",
        "        new_in_channels = new_mask.sum().item()\n",
        "        new_layer = nn.Conv2d(new_in_channels, layer.out_channels,\n",
        "                              kernel_size=layer.kernel_size, stride=layer.stride,\n",
        "                              padding=layer.padding, bias=(layer.bias is not None))\n",
        "\n",
        "        new_layer.weight.data = layer.weight.data[:, new_mask, :, :].clone()\n",
        "        if layer.bias is not None:\n",
        "            new_layer.bias.data = layer.bias.data.clone()\n",
        "        replace_layer_in_model(model, layer, new_layer)\n",
        "    elif block is model.layers[23]:\n",
        "        layer = model.layers[25].conv\n",
        "        num_in_channels = layer.in_channels\n",
        "        new_mask = torch.ones(num_in_channels, dtype=torch.bool)\n",
        "        new_mask[:len(mask)] = mask\n",
        "        new_in_channels = new_mask.sum().item()\n",
        "        new_layer = nn.Conv2d(new_in_channels, layer.out_channels,\n",
        "                              kernel_size=layer.kernel_size, stride=layer.stride,\n",
        "                              padding=layer.padding, bias=(layer.bias is not None))\n",
        "\n",
        "        new_layer.weight.data = layer.weight.data[:, new_mask, :, :].clone()\n",
        "        if layer.bias is not None:\n",
        "            new_layer.bias.data = layer.bias.data.clone()\n",
        "        replace_layer_in_model(model, layer, new_layer)\n",
        "\n",
        "\n",
        "def prune_concat_layer(model, block, mask):\n",
        "    if block is model.layers[6]:\n",
        "        layer = model.layers[25].conv\n",
        "        num_in_channels = layer.in_channels\n",
        "        new_mask = torch.ones(num_in_channels, dtype=torch.bool)\n",
        "        new_mask[-len(mask):] = mask\n",
        "        new_in_channels = new_mask.sum().item()\n",
        "        new_layer = nn.Conv2d(new_in_channels, layer.out_channels,\n",
        "                              kernel_size=layer.kernel_size, stride=layer.stride,\n",
        "                              padding=layer.padding, bias=(layer.bias is not None))\n",
        "\n",
        "        new_layer.weight.data = layer.weight.data[:, new_mask, :, :].clone()\n",
        "        if layer.bias is not None:\n",
        "            new_layer.bias.data = layer.bias.data.clone()\n",
        "        replace_layer_in_model(model, layer, new_layer)\n",
        "    elif block is model.layers[8]:\n",
        "        layer = model.layers[18].conv\n",
        "        num_in_channels = layer.in_channels\n",
        "        new_mask = torch.ones(num_in_channels, dtype=torch.bool)\n",
        "        new_mask[-len(mask):] = mask\n",
        "        new_in_channels = new_mask.sum().item()\n",
        "        new_layer = nn.Conv2d(new_in_channels, layer.out_channels,\n",
        "                              kernel_size=layer.kernel_size, stride=layer.stride,\n",
        "                              padding=layer.padding, bias=(layer.bias is not None))\n",
        "\n",
        "        new_layer.weight.data = layer.weight.data[:, new_mask, :, :].clone()\n",
        "        if layer.bias is not None:\n",
        "            new_layer.bias.data = layer.bias.data.clone()\n",
        "        replace_layer_in_model(model, layer, new_layer)\n",
        "\n",
        "\n",
        "# New function to prune the first layer in a residual block\n",
        "def prune_first_layer(block_idx, seq_idx, prune_percentages, model, loss_fn, scaler, scaled_anchors, train_loader, validation_loader, device, original_loss, post_pruned_threshold, post_retrained_threshold, scale_pred=False):\n",
        "    count = 0\n",
        "    prune_perc = prune_percentages[count]\n",
        "    while True:\n",
        "        torch.save(model, 'model_to_revert.pth')\n",
        "        if scale_pred:\n",
        "            block = model.layers[block_idx]\n",
        "            conv_layer = block.pred[0].conv\n",
        "            bn_layer = block.pred[0].bn\n",
        "            next_layer = block.pred[1].conv\n",
        "        else:\n",
        "            block = model.layers[block_idx]\n",
        "            sequential = block.layers[seq_idx]\n",
        "            conv_layer = sequential[0].conv\n",
        "            bn_layer = sequential[0].bn\n",
        "            next_layer = sequential[1].conv\n",
        "\n",
        "        num_filters_to_prune = max(1, int(prune_perc * conv_layer.out_channels))\n",
        "        if conv_layer.out_channels <= num_filters_to_prune:\n",
        "            print(\"First layer pruning stopped due to maximum pruning\")\n",
        "            break\n",
        "\n",
        "        print(\"-------Pruning Start(main)-------\")\n",
        "        if CRITERION == \"Taylor\":\n",
        "            print(\"-----Taylor Pruning-----\")\n",
        "            new_conv1, mask = taylor_prune_conv_layer(conv_layer, model, loss_fn, validation_loader, device, scaled_anchors, num_filters_to_prune)\n",
        "        elif CRITERION == \"L1\":\n",
        "            print(\"-----L1 Pruning-----\")\n",
        "            new_conv1, mask = prune_conv_layer(conv_layer, num_filters_to_prune)\n",
        "        else: # prune filters randomly\n",
        "            print(\"-----Random Pruning-----\")\n",
        "            new_conv1, mask = random_prune_conv_layer(conv_layer, num_filters_to_prune)\n",
        "        # new_conv1, mask = prune_conv_layer(conv_layer, num_filters_to_prune)\n",
        "\n",
        "        new_bn = update_batchnorm_for_pruned_conv(bn_layer, mask).to(device)\n",
        "        new_conv2 = update_next_layer(next_layer, mask, model, loss_fn, validation_loader, device, scaled_anchors)\n",
        "        replace_layer_in_model(model, conv_layer, new_conv1)\n",
        "        print(\"before pruning:\", conv_layer)\n",
        "        replace_layer_in_model(model, bn_layer, new_bn)\n",
        "        replace_layer_in_model(model, next_layer, new_conv2)\n",
        "        conv_layer = get_current_layer(model, new_conv1)\n",
        "        print(\"After pruning:\", conv_layer)\n",
        "        bn_layer = get_current_layer(model, new_bn)\n",
        "        next_layer = get_current_layer(model, new_conv2)\n",
        "        # Evaluate the pruned model\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "        early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
        "        pruned_loss = validate_fn(validation_loader, model, loss_fn, scaled_anchors)\n",
        "        print(f\"Loss before pruning : {original_loss:.4f}\")\n",
        "        print(f\"Loss after pruning : {pruned_loss:.4f}\")\n",
        "        epoch = 0\n",
        "        if pruned_loss > original_loss * post_pruned_threshold:\n",
        "            # Retrain the model\n",
        "            while True:\n",
        "                print(f\"Currently epoch {epoch}\")\n",
        "                mean_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
        "                mean_val_loss = validate_fn(validation_loader, model, loss_fn, scaled_anchors)\n",
        "                # Early Stopping logic\n",
        "                early_stopping(mean_val_loss)\n",
        "                if early_stopping.early_stop:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                    early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
        "                    break\n",
        "                if early_stopping.best_loss <= original_loss * post_retrained_threshold:\n",
        "                    early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
        "                    print(\"Retrained loss reached the threshold\")\n",
        "                    break\n",
        "                epoch += 1\n",
        "            # Evaluate after retraining\n",
        "            # retrained_loss = evaluate_loss(model, validation_loader, criterion, device)\n",
        "            retrained_loss = validate_fn(validation_loader, model, loss_fn, scaled_anchors)\n",
        "            print(f\"Loss after retraining: {retrained_loss:.4f}\")\n",
        "\n",
        "            if retrained_loss <= original_loss * post_retrained_threshold:\n",
        "                # Pruning successful\n",
        "                print(\"-----Retraining Succeed-----\")\n",
        "                print(f\"Continuing to prune {seq_idx} with the same prune_perc: {prune_perc:.4f}\")\n",
        "            else:\n",
        "                # Pruning unsuccessful, stop pruning this layer\n",
        "                # Revert the change\n",
        "                print(\"-----Retraining Failed-----\")\n",
        "                if count < len(prune_percentages) - 1:\n",
        "                    count += 1\n",
        "                    prune_perc = prune_percentages[count]\n",
        "                    print(f\"Prune Percentage is changed to {prune_perc}\")\n",
        "                    model = torch.load('model_to_revert.pth')\n",
        "                else:\n",
        "                    print(f\"{block_idx} {seq_idx} pruning done due to hitting max number of pruning percentage\")\n",
        "                    model = torch.load('model_to_revert.pth')\n",
        "                    break\n",
        "\n",
        "        else:\n",
        "            # Pruning successful without retraining, continue pruning this layer with reduced percentage\n",
        "            print(f\"Continuing to prune first layer_block with new prune_perc: {prune_perc:.4f} (no retraining)\")\n",
        "            print(\"-------------\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def prune_residual_block(block_idx, block, prune_percentages, model, loss_fn, scaler, scaled_anchors, train_loader, validation_loader, device, original_loss, post_pruned_threshold, post_retrained_threshold):\n",
        "    conv_layer = None\n",
        "    bn_layer = None\n",
        "    next_layer = None\n",
        "    for seq_idx, sequential in enumerate(block.layers):\n",
        "        # prune only first CNNBlock\n",
        "        print(f\"{seq_idx} block is currently being pruned\")\n",
        "        # conv_layer = sequential[0].conv\n",
        "        # bn_layer = sequential[0].bn\n",
        "        # next_layer = sequential[1].conv\n",
        "\n",
        "        model = prune_first_layer(block_idx, seq_idx, prune_percentages, model, loss_fn, scaler, scaled_anchors, train_loader, validation_loader, device, original_loss, post_pruned_threshold, post_retrained_threshold)\n",
        "        print(\"------------After updated-------------\")\n",
        "        print((model.layers[block_idx]).layers[seq_idx])\n",
        "        print(\"--------------------------------------\")\n",
        "        # else:\n",
        "        #     # first CNNBlock\n",
        "        #     first_conv_layer = sequential[0].conv\n",
        "        #     first_bn_layer = sequential[0].bn\n",
        "        #     first_next_layer = sequential[1].conv\n",
        "        #     prune_first_layer(first_conv_layer, first_bn_layer, first_next_layer, prune_perc, model, train_loader, validation_loader, trainer, lightning_model, device, original_loss, prune_decay, post_pruned_threshold, post_retrained_threshold)\n",
        "        #     # second CNNBlock\n",
        "        #     second_conv_layer = sequential[1].conv\n",
        "        #     second_bn_layer = sequential[1].bn\n",
        "        #     second_next_layer = block.layers[idx+1][0].conv\n",
        "        #     prune_first_layer(second_conv_layer, second_bn_layer, second_next_layer, prune_perc, model, train_loader, validation_loader, trainer, lightning_model, device, original_loss, prune_decay, post_pruned_threshold, post_retrained_threshold)\n",
        "\n",
        "    print(\"----------Final ResidualBlock---------\")\n",
        "    print(block)\n",
        "    return model\n",
        "def evaluate_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iQcSfqdVRfp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Main for pruning (New Version)\n",
        "import copy\n",
        "def main():\n",
        "\n",
        "    model = torch.load('/content/drive/MyDrive/models/second_model_complete.pth')\n",
        "    model = model.to(DEVICE)\n",
        "    # parameter counts\n",
        "    # Total parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # Trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"Total Parameters: {total_params}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params}\")\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    loss_fn = YoloLoss()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    train_loader, test_loader, train_eval_loader = get_loaders(\n",
        "        train_images_list, train_labels_list, val_images_list, val_labels_list\n",
        "    )\n",
        "\n",
        "    scaled_anchors = (\n",
        "        torch.tensor(ANCHORS)\n",
        "        * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    epoch_losses = []  # List to store loss per epoch\n",
        "    val_losses = []\n",
        "    early_stopping = EarlyStopping(patience=5, min_delta=0.01)  # Set patience and min_delta here\n",
        "\n",
        "    initial_loss = validate_fn(train_eval_loader, model, loss_fn, scaled_anchors)\n",
        "    print(f\"Initial validation loss: {initial_loss:.4f}\")\n",
        "\n",
        "    # Set pruning parameters\n",
        "    post_pruned_threshold = 1.1  # 20% increase in loss\n",
        "    post_retrained_threshold = 1.1  # 10% increase in loss\n",
        "    prev_model = None\n",
        "    prune_percentages = [0.7, 0.5, 0.2]\n",
        "    # Iterative pruning and retraining\n",
        "    for idx, layer in enumerate(model.layers):\n",
        "\n",
        "        if isinstance(layer, ResidualBlock):\n",
        "            print(f\"{idx} is pruning now\")\n",
        "            print(\"---------ResidualBlock Pruning---------\")\n",
        "            model = prune_residual_block(idx, layer, prune_percentages, model, loss_fn, scaler, scaled_anchors, train_loader, train_eval_loader, DEVICE, initial_loss, post_pruned_threshold, post_retrained_threshold)\n",
        "            print(\"----After pruning ResidualBlock----\")\n",
        "            print(model)\n",
        "            print(\"-----------------------------------\")\n",
        "            print(\"-----------------------------------\")\n",
        "        elif isinstance(layer, CNNBlock):\n",
        "            # only prune a Conv2d not in the block\n",
        "            print(\"---------CNNBlock Pruning---------\")\n",
        "            count = 0\n",
        "            prune_perc = prune_percentages[count]\n",
        "            while True:\n",
        "                print(f\"{idx} is pruning now\")\n",
        "                #  save model to revert\n",
        "                torch.save(model, 'model_to_revert.pth')\n",
        "                layer = model.layers[idx]\n",
        "                conv_layer = layer.conv\n",
        "                if layer.use_bn_act:\n",
        "                    bn_layer = layer.bn\n",
        "                else:\n",
        "                    bn_layer = None\n",
        "\n",
        "                # Prune the layer\n",
        "                num_filters_to_prune = max(1, int(prune_perc * conv_layer.out_channels))\n",
        "                if conv_layer.out_channels <= num_filters_to_prune:\n",
        "                    print(f\"{idx} {layer} pruning done due to hitting max pruning filters\")\n",
        "                    break\n",
        "                print(\"-------Pruning Start(main)-------\")\n",
        "                if CRITERION == \"Taylor\":\n",
        "                    print(\"-----Taylor Pruning-----\")\n",
        "                    new_layer, mask = taylor_prune_conv_layer(conv_layer, model, loss_fn, train_eval_loader, DEVICE, scaled_anchors, num_filters_to_prune)\n",
        "                    prev_model = copy.deepcopy(model)\n",
        "                elif CRITERION == \"L1\":\n",
        "                    print(\"-----L1 Pruning-----\")\n",
        "                    new_layer, mask = prune_conv_layer(conv_layer, num_filters_to_prune)\n",
        "                else: # prune filters randomly\n",
        "                    print(\"-----Random Pruning-----\")\n",
        "                    new_layer, mask = random_prune_conv_layer(conv_layer, num_filters_to_prune)\n",
        "                # Replace the conv_layer in the model\n",
        "                replace_layer_in_model(model, conv_layer, new_layer)\n",
        "                print(\"before pruning: \", conv_layer)\n",
        "                # refesh the current layer\n",
        "                conv_layer = get_current_layer(model, new_layer)\n",
        "                print(\"After pruning: \", conv_layer)\n",
        "\n",
        "                # Update its batchNorm layer\n",
        "                if bn_layer is not None:\n",
        "                    new_bn_layer = update_batchnorm_for_pruned_conv(bn_layer, mask).to(DEVICE)\n",
        "                    replace_layer_in_model(model, bn_layer, new_bn_layer)\n",
        "                    # refesh the current bn\n",
        "                    bn_layer = get_current_layer(model, new_bn_layer)\n",
        "                # Update its next layer if there is\n",
        "                if idx+1 < len(model.layers):\n",
        "                    if idx == 16 or idx == 23:\n",
        "                        update_concat_layer(model, layer, mask)\n",
        "                    else:\n",
        "                        next_layer = model.layers[idx+1]\n",
        "                        if isinstance(next_layer, ScalePrediction) and idx != 28:\n",
        "                            one_more_next_layer = model.layers[idx+2]\n",
        "                            one_more_updated_next_layer = update_next_layer(one_more_next_layer, mask, model, loss_fn, train_eval_loader, DEVICE, scaled_anchors, prev_model).to(DEVICE)\n",
        "                            replace_layer_in_model(model, one_more_next_layer, one_more_updated_next_layer)\n",
        "                        updated_next_layer = update_next_layer(next_layer, mask, model, loss_fn, train_eval_loader, DEVICE, scaled_anchors, prev_model).to(DEVICE)\n",
        "                        replace_layer_in_model(model, next_layer, updated_next_layer)\n",
        "\n",
        "\n",
        "                # Evaluate the pruned model\n",
        "                optimizer = optim.Adam(\n",
        "                    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "                )\n",
        "                early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
        "                pruned_loss = validate_fn(train_eval_loader, model, loss_fn, scaled_anchors)\n",
        "                print(f\"Initial Loss: {initial_loss:.4f}\")\n",
        "                print(f\"Loss After pruning {idx}: {pruned_loss:.4f}\")\n",
        "                epoch = 0\n",
        "                if pruned_loss > initial_loss * post_pruned_threshold:\n",
        "                    # Retrain the model\n",
        "                    while True:\n",
        "                        print(f\"Currently epoch {epoch}\")\n",
        "                        mean_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
        "                        mean_val_loss = validate_fn(train_eval_loader, model, loss_fn, scaled_anchors)\n",
        "                        # Early Stopping logic\n",
        "                        early_stopping(mean_val_loss)\n",
        "                        if early_stopping.early_stop:\n",
        "                            print(\"Early stopping triggered\")\n",
        "                            early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
        "                            break\n",
        "                        if early_stopping.best_loss <= initial_loss * post_retrained_threshold:\n",
        "                            early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
        "                            print(\"Retrained loss reached the threshold\")\n",
        "                            break\n",
        "                        epoch += 1\n",
        "\n",
        "                    # Evaluate after retraining\n",
        "                    retrained_loss = validate_fn(train_eval_loader, model, loss_fn, scaled_anchors)\n",
        "                    print(f\"Loss after retraining: {retrained_loss:.4f}\")\n",
        "\n",
        "                    if retrained_loss <= initial_loss * post_retrained_threshold:\n",
        "                        # Pruning successful\n",
        "                        print(\"-----Retraining Succeed-----\")\n",
        "                        print(f\"Continuing to prune {idx} with the same prune_perc: {prune_perc:.4f}\")\n",
        "                    else:\n",
        "                        # Pruning unsuccessful, stop pruning this layer\n",
        "                        # Revert the change\n",
        "                        print(\"-----Retraining Failed-----\")\n",
        "                        if count < len(prune_percentages) - 1:\n",
        "                            count += 1\n",
        "                            prune_perc = prune_percentages[count]\n",
        "                            print(f\"Prune Percentage is changed to {prune_perc}\")\n",
        "                            model = torch.load('model_to_revert.pth')\n",
        "                        else:\n",
        "                            print(f\"{idx} {layer} pruning done due to hitting max number of pruning percentage\")\n",
        "                            model = torch.load('model_to_revert.pth')\n",
        "                            break\n",
        "\n",
        "                else:\n",
        "                    # Pruning successful without retraining, continue pruning this layer with reduced percentage\n",
        "                    print(f\"Continuing to prune {idx} with new prune_perc: {prune_perc:.4f} (no retraining)\")\n",
        "\n",
        "            print(\"----After pruning CNNBlock----\")\n",
        "            print(model)\n",
        "            print(\"------------------------------\")\n",
        "            print(\"------------------------------\")\n",
        "\n",
        "        elif isinstance(layer, ScalePrediction):\n",
        "            print(f\"{idx} is pruning now\")\n",
        "            # conv_layer = layer.pred[0].conv\n",
        "            # bn_layer = layer.pred[0].bn\n",
        "            # next_layer = layer.pred[1].conv\n",
        "            scale_pred = True\n",
        "            seq_idx = 0\n",
        "            model = prune_first_layer(idx, seq_idx, prune_percentages, model, loss_fn, scaler, scaled_anchors, train_loader, train_eval_loader, DEVICE, initial_loss, post_pruned_threshold, post_retrained_threshold, scale_pred)\n",
        "            print(\"----After pruning ScalePrediction----\")\n",
        "            print(model)\n",
        "            print(\"-------------------------------------\")\n",
        "            print(\"-------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(\"\\n--------\\n\")\n",
        "        print(\"\\n--------\\n\")\n",
        "\n",
        "\n",
        "    # Final evaluation\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total number of parameters: {total_params}\")\n",
        "\n",
        "    # Save the pruned model\n",
        "    torch.save(model, 'L1_pruned_resnet.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ca9mrMpbgpx"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQHYAdr1GVwd"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk0m76gmhZCg"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_IU2OB-hx90"
      },
      "outputs": [],
      "source": [
        "# save_checkpoint(model, optimizer, filename=f\"pruned_checkpoint.pth.tar\")\n",
        "# @title Save model on drive\n",
        "\n",
        "def copy_model_file(source_path, destination_path):\n",
        "    try:\n",
        "        shutil.copy(source_path, destination_path)\n",
        "        print(f'Tar file copied to: {destination_path}')\n",
        "    except Exception as e:\n",
        "        print(f'An error occurred: {e}')\n",
        "\n",
        "# Source path of the tar file\n",
        "source_tar_file_path = '/content/L1_pruned_resnet.pth'\n",
        "\n",
        "# Destination path in Google Drive\n",
        "drive_tar_folder_path = '/content/drive/MyDrive/models/'\n",
        "\n",
        "copy_model_file(source_tar_file_path, drive_tar_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title VIDEO FRAME TEST\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import time\n",
        "\n",
        "def process_video(video_path, model, device, conf_threshold=0.5, nms_threshold=0.4, time_limit=240):\n",
        "    # Load the video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video properties\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter('output.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "    # Prepare image transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((416, 416)),  # Adjust size according to your model\n",
        "    ])\n",
        "\n",
        "    frame_count = 0\n",
        "    total_time = 0\n",
        "    total_latency = 0\n",
        "    start_time_total = time.time()\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Preprocess the frame\n",
        "        input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        input_img = transform(input_img).unsqueeze(0).to(device)\n",
        "\n",
        "        # Run inference\n",
        "        with torch.no_grad():\n",
        "            predictions = model(input_img)\n",
        "\n",
        "        # Post-process predictions\n",
        "        # This part needs to be implemented based on your model's output format\n",
        "        # Need to apply non-max suppression and filter by confidence\n",
        "\n",
        "        # Draw bounding boxes on the frame\n",
        "        # This part needs to be implemented based on your post-processing results\n",
        "\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "        total_latency += latency\n",
        "        total_time = end_time - start_time_total\n",
        "\n",
        "        # Calculate and display FPS and latency\n",
        "        if frame_count % 10 == 0:  # Update every 10 frames\n",
        "            avg_fps = frame_count / total_time\n",
        "            avg_latency = total_latency / frame_count\n",
        "            fps_text = f\"FPS: {avg_fps:.2f}\"\n",
        "            latency_text = f\"Latency: {avg_latency*1000:.2f} ms\"\n",
        "            cv2.putText(frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "            cv2.putText(frame, latency_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Write the frame\n",
        "        out.write(frame)\n",
        "\n",
        "        # Check if we've reached the time limit\n",
        "        if total_time >= time_limit:\n",
        "            print(f\"Reached time limit of {time_limit} seconds. Stopping processing.\")\n",
        "            break\n",
        "\n",
        "    # Print final average FPS and latency\n",
        "    avg_fps = frame_count / total_time\n",
        "    avg_latency = total_latency / frame_count\n",
        "    print(f\"Average FPS: {avg_fps:.2f}\")\n",
        "    print(f\"Average Latency: {avg_latency*1000:.2f} ms\")\n",
        "    print(f\"Total frames processed: {frame_count}\")\n",
        "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Set up the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 1  # Adjust based on your model\n",
        "model = torch.load('/content/drive/MyDrive/models/second_pruned_resnet.pth', map_location=device).to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Process the video\n",
        "video_path = '/content/drive/MyDrive/datasets/Kvasir_Capsule.mp4'\n",
        "process_video(video_path, model, device)"
      ],
      "metadata": {
        "id": "dsI2ncGLUao3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}